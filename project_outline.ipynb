{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from bert_score import score\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categories(directory_path):\n",
    "    \"\"\"\n",
    "    Analyzes the categories in the dataset and counts documents in each category.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing JSON files\n",
    "    \n",
    "    Returns:\n",
    "        None: Prints the analysis results\n",
    "    \"\"\"\n",
    "    path = Path(directory_path)\n",
    "    json_files = list(path.glob('*.json'))\n",
    "    print(f\"The # of discovered JSON files: {len(json_files)}\")\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found in the directory.\")\n",
    "        return\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for file_idx, json_file in enumerate(json_files):\n",
    "        print(f\"\\n--- Analyzing file {file_idx+1}/{len(json_files)}: {json_file.name} ---\")\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if 'documents' not in data:\n",
    "                    print(f\"No 'documents' field found in {json_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                total_docs = len(data['documents'])\n",
    "                print(f\"Total documents in this file: {total_docs}\")\n",
    "                \n",
    "                # Count documents by category\n",
    "                categories = [doc.get('category', 'Unknown') for doc in data['documents']]\n",
    "                category_counts = Counter(categories)\n",
    "                \n",
    "                # Print category distribution\n",
    "                print(\"\\nCategory distribution:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"{'Category':<20} | {'Count':<10} | {'Percentage':<10}\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                    percentage = (count / total_docs) * 100\n",
    "                    print(f\"{category:<20} | {count:<10} | {percentage:.2f}%\")\n",
    "                \n",
    "                # Additional statistics for a more detailed view\n",
    "                # Check subcategories (media_sub_type) if needed\n",
    "                print(\"\\nTop 5 media subtypes:\")\n",
    "                media_subtypes = [doc.get('media_sub_type', 'Unknown') for doc in data['documents']]\n",
    "                subtype_counts = Counter(media_subtypes).most_common(5)\n",
    "                for subtype, count in subtype_counts:\n",
    "                    print(f\"- {subtype}: {count} documents\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not parse {json_file.name} as valid JSON\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {json_file.name}: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_structure(directory_path):\n",
    "    \"\"\"The function which checks the internal structure of the dataset\"\"\"\n",
    "    path = Path(directory_path)\n",
    "    json_files = list(path.glob('*.json'))\n",
    "    print(f\"The # of discovered JSON files: {len(json_files)}\")\n",
    "    \n",
    "    if json_files:\n",
    "        with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            print(\"\\nData type:\", type(data))\n",
    "            \n",
    "            if 'documents' in data:\n",
    "                # Only filter the economy category news\n",
    "                econ_fin_docs = [doc for doc in data['documents'] \n",
    "                               if doc['category'] in ['경제']]\n",
    "                \n",
    "                print(\"\\n=== Status of econ news data ===\")\n",
    "                print(f\"The # of documents related to economics: {len(econ_fin_docs)}\")\n",
    "                \n",
    "                if econ_fin_docs:\n",
    "                    print(\"\\nThe sample of the first econ news:\")\n",
    "                    # 제한 없이 전체 출력 ([:1000] 부분 제거)\n",
    "                    print(json.dumps(econ_fin_docs[0], indent=2, ensure_ascii=False))\n",
    "                    print(\"\\nKey structure of such document:\")\n",
    "                    print(list(econ_fin_docs[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "news_train_file_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Training/신문기사_train_original\"\n",
    "news_val_file_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Validation/신문기사_valid_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The # of discovered JSON files: 1\n",
      "\n",
      "--- Analyzing file 1/1: train_original.json ---\n",
      "Total documents in this file: 243983\n",
      "\n",
      "Category distribution:\n",
      "----------------------------------------\n",
      "Category             | Count      | Percentage\n",
      "----------------------------------------\n",
      "종합                   | 177558     | 72.77%\n",
      "경제                   | 23938      | 9.81%\n",
      "사회                   | 17650      | 7.23%\n",
      "정치                   | 16389      | 6.72%\n",
      "스포츠                  | 5174       | 2.12%\n",
      "IT,과학                | 1931       | 0.79%\n",
      "교육/입시/NIE            | 1190       | 0.49%\n",
      "부동산                  | 74         | 0.03%\n",
      "보건/의료                | 55         | 0.02%\n",
      "기업                   | 17         | 0.01%\n",
      "북한/한반도정세             | 5          | 0.00%\n",
      "선거                   | 2          | 0.00%\n",
      "\n",
      "Top 5 media subtypes:\n",
      "- 지역지: 169724 documents\n",
      "- 경제지: 68388 documents\n",
      "- 전문지: 5871 documents\n",
      "None\n",
      "The # of discovered JSON files: 1\n",
      "\n",
      "--- Analyzing file 1/1: valid_original.json ---\n",
      "Total documents in this file: 30122\n",
      "\n",
      "Category distribution:\n",
      "----------------------------------------\n",
      "Category             | Count      | Percentage\n",
      "----------------------------------------\n",
      "종합                   | 13666      | 45.37%\n",
      "IT,과학                | 10997      | 36.51%\n",
      "경제                   | 3403       | 11.30%\n",
      "사회                   | 1503       | 4.99%\n",
      "스포츠                  | 392        | 1.30%\n",
      "정치                   | 161        | 0.53%\n",
      "\n",
      "Top 5 media subtypes:\n",
      "- 경제지: 21810 documents\n",
      "- 중앙지: 8312 documents\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the # of categories of the dataset\n",
    "print(analyze_categories(news_train_file_path))\n",
    "print(analyze_categories(news_val_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The # of discovered JSON files: 1\n",
      "\n",
      "Data type: <class 'dict'>\n",
      "\n",
      "=== Status of econ news data ===\n",
      "The # of documents related to economics: 23938\n",
      "\n",
      "The sample of the first econ news:\n",
      "{\n",
      "  \"id\": \"291514704\",\n",
      "  \"category\": \"경제\",\n",
      "  \"media_type\": \"online\",\n",
      "  \"media_sub_type\": \"지역지\",\n",
      "  \"media_name\": \"광양신문\",\n",
      "  \"size\": \"small\",\n",
      "  \"char_count\": \"886\",\n",
      "  \"publish_date\": \"2018-01-12 18:42:06\",\n",
      "  \"title\": \"여수광양항만공사, 광양항 배후단지‘투자유치’시동\",\n",
      "  \"text\": [\n",
      "    [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"sentence\": \"중국 투자의향기업 직접 방문·투자협약 체결 협의\",\n",
      "        \"highlight_indices\": \"10,12\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 1,\n",
      "        \"sentence\": \"이성훈 sinawi@hanmail.net\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 2,\n",
      "        \"sentence\": \"여수광양항만공사(사장 방희석)는 14일부터 20일까지 광양항 배후단지 투자 유치를 위해 CEO가 직접 참여하는 투자유치 활동을 중국 중남부지역에서 펼친다.\",\n",
      "        \"highlight_indices\": \"36,38;54,56\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 3,\n",
      "        \"sentence\": \"광양만권경제자유구역청과 합동으로 진행되는 이번 투자유치활동은 방희석 사장이 직접 중국 현지의 투자의향기업을 방문해 광양항 및 배후단지의 장점 등을 소개하고 투자협약(MOU)을 체결하는 방식으로 진행된다.\",\n",
      "        \"highlight_indices\": \"42,44;68,69\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 4,\n",
      "        \"sentence\": \"방 사장은 먼저 광양항 서측배후단지 푸드존 투자유치를 위해 중국 운남성 소재 커피 원재료 공급업체인 운남허메이격치(주)를 방문해 광양항 배후단지 투자협약를 체결할 예정이다.\",\n",
      "        \"highlight_indices\": \"6,8;17,19;78,80\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 5,\n",
      "        \"sentence\": \"이어 한중일 콜드체인 거점 구축을 위해 중국 안후이성 소재 냉동냉장창고 업체인 바이란식품(주)을 방문해 투자협약을 맺기로 했다.\",\n",
      "        \"highlight_indices\": \"0,1\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 6,\n",
      "        \"sentence\": \"중국의 다른 입주의향기업들도 방문해 투자유치 활동을 진행할 계획이다.\",\n",
      "        \"highlight_indices\": \"4,6\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 7,\n",
      "        \"sentence\": \"이번 투자유치 활동은 지난해 9월 경제청과 공사가 합동으로 중국지역 투자유치활동을 통해 투자의향기업을 선정했던 후속 조치로, 이번 방문을 통해 광양항 배후단지에 실질적인 투자가 이뤄질 수 있도록 하기 위한 것이다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 8,\n",
      "        \"sentence\": \"방희석 사장은“이번 중국 방문은 공사, 경제청이 합동으로 광양항 배후단지에 대한 투자유치를 이끌기 위한 것”이라며“광양항 서측배후단지 푸드존에 중국기업이 입주하는 실질적인 투자유치 성과가 있을 것으로 기대된다”고 배경을 설명했다.\",\n",
      "        \"highlight_indices\": \"72,74\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 9,\n",
      "        \"sentence\": \"방 사장은“광양항 배후단지를 한·중·일 콜드체인 거점 및 식품산업 클러스터로 발전시키는 충분한 계기가 될 것”이라며“앞으로도 광양항 배후단지 활성화를 위해 CEO가 직접 투자유치 활동에 나서는 등 가시적인 효과를 낼 수 있도록 최선을 다하겠다”고 밝혔다.\",\n",
      "        \"highlight_indices\": \"30,31;76,78;92,94;131,132\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"annotator_id\": 12,\n",
      "  \"document_quality_scores\": {\n",
      "    \"readable\": 4,\n",
      "    \"accurate\": 4,\n",
      "    \"informative\": 4,\n",
      "    \"trustworthy\": 4\n",
      "  },\n",
      "  \"extractive\": [\n",
      "    2,\n",
      "    3,\n",
      "    7\n",
      "  ],\n",
      "  \"abstractive\": [\n",
      "    \"여수광양항만공사는 광양만권경제자유구역청과 합동으로 직접 중국 현지의 투자의향기업을 방문해 광양항 배후단지 투자협약(MOU)을 체결하여 실질적인 투자가 이뤄지도록 활동을 진행한다.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Key structure of such document:\n",
      "['id', 'category', 'media_type', 'media_sub_type', 'media_name', 'size', 'char_count', 'publish_date', 'title', 'text', 'annotator_id', 'document_quality_scores', 'extractive', 'abstractive']\n",
      "None\n",
      "The # of discovered JSON files: 1\n",
      "\n",
      "Data type: <class 'dict'>\n",
      "\n",
      "=== Status of econ news data ===\n",
      "The # of documents related to economics: 3403\n",
      "\n",
      "The sample of the first econ news:\n",
      "{\n",
      "  \"id\": \"340627465\",\n",
      "  \"category\": \"경제\",\n",
      "  \"media_type\": \"online\",\n",
      "  \"media_sub_type\": \"경제지\",\n",
      "  \"media_name\": \"한국경제\",\n",
      "  \"size\": \"medium\",\n",
      "  \"char_count\": \"1389\",\n",
      "  \"publish_date\": \"2019-04-08 17:49:00\",\n",
      "  \"title\": \"\\\"SK·알파벳 벤치마킹해야\\\"...KB운용, KMH에 주주서한\",\n",
      "  \"text\": [\n",
      "    [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"sentence\": \"[ 임근호 기자 ] \\\"SK(주)와 미국 알파벳(구글 지주회사)의 간결한 지배구조를 배워라.\\\"\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 1,\n",
      "        \"sentence\": \"기업 지배구조 개선 등을 통해 높은 수익률을 올리는 것을 목표로 하는 공모펀드 'KB주주가치포커스'를 운용 중인 KB자산운용이 코스닥 상장사 KMH에 이런 내용을 담은 주주서한을 최근 보냈다.\",\n",
      "        \"highlight_indices\": \"84,86\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 2,\n",
      "        \"sentence\": \"KB운용은 KMH 지분 10.24%를 보유한 주요 주주다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 3,\n",
      "        \"sentence\": \"SK와 알파벳은 신규 투자는 지주회사가 전담하고, 자회사는 본업에만 충실할 수 있도록 모범적인 지배구조를 짰다는 게 KB운용 평가다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 4,\n",
      "        \"sentence\": \"반면 KMH가 성공적 인수합병(M&A)으로 2013년부터 6년 동안 지배주주 순이익이 280% 늘었는 데도 주가가 35% 오르는 데 그친 것은 복잡한 지배구조와 나빠진 현금흐름 때문이라고 분석했다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 5,\n",
      "        \"sentence\": \"2000년 설립돼 방송 송출과 채널 사업을 하는 KMH는 2011년 코스닥시장에 상장했다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 6,\n",
      "        \"sentence\": \"2013년부터 M&A를 본격화해 2012년 2개에 불과했던 종속회사가 25개로 급증했다.\",\n",
      "        \"highlight_indices\": \"28,30\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 7,\n",
      "        \"sentence\": \"케이엠하이텍, KMH신라레저, 광명역환승파크 등이다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 8,\n",
      "        \"sentence\": \"KB운용은 적자 회사나 법정 관리·회생 절차 중인 회사 등을 싸게 인수한 뒤 과감한 구조조정으로 흑자로 전환시키는 게 KMH의 강점이라고 평가했다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 9,\n",
      "        \"sentence\": \"2012년 620억원이던 KMH 매출은 작년 1926억원으로, 영업이익은 121억원에서 346억원으로 각각 3배가량 늘었다.\",\n",
      "        \"highlight_indices\": \"57,59\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 10,\n",
      "        \"sentence\": \"그러나 모회사 KMH뿐 아니라 상장 자회사들도 M&A에 참여해 인수기업의 지분을 나눠 가지면서 지배구조가 복잡하게 얽히게 됐다.\",\n",
      "        \"highlight_indices\": \"0,3\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 11,\n",
      "        \"sentence\": \"KB운용은 인수한 기업의 가치가 분산돼 그만큼 KMH 주가는 할인돼 거래되고 있다며 자회사들이 배당을 하지 않아 KMH의 현금흐름이 나빠지고 있는 점도 문제라고 지적했다.\",\n",
      "        \"highlight_indices\": \"22,25\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 12,\n",
      "        \"sentence\": \"KB운용이 KMH에 좋은 지배구조 사례로 든 것이 SK와 알파벳이다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 13,\n",
      "        \"sentence\": \"SK는 SK그룹의 투자 전문 지주회사로, SK텔레콤과 SK이노베이션 등으로부터 수천억원 규모의 배당을 받아 SK바이오텍, SK바이오팜, SK머티리얼즈 등 그룹의 미래 성장동력에 투자하고 있다.\",\n",
      "        \"highlight_indices\": \"\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 14,\n",
      "        \"sentence\": \"KB운용은 SK는 주요 자회사들이 적극적으로 배당을 한 덕분에 2017년에만 총 6900억원을 받아 현금흐름이 우수하고, 신규 사업을 벌여도 이 회사를 정점으로 한 지배구조가 흐트러지지 않는다고 분석했다.\",\n",
      "        \"highlight_indices\": \"43,44;79,80\"\n",
      "      },\n",
      "      {\n",
      "        \"index\": 15,\n",
      "        \"sentence\": \"알파벳도 이 회사가 신규 투자에 전념하고 검색업체 구글, 인공지능(AI) 연구회사 딥마인드, 자율주행차 개발업체 웨이모 등은 각자 본업에만 집중하는 구조라는 설명이다.\",\n",
      "        \"highlight_indices\": \"5,6\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"index\": 16,\n",
      "        \"sentence\": \"이에 대해 KMH는 투자 주체로서의 역할을 보다 확실히 확립하겠다면서 사업 시너지와 현금 유동성을 고려해 투자 주체와 방식을 정하겠다고 답했다.\",\n",
      "        \"highlight_indices\": \"24,26;27,30\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"annotator_id\": 2411,\n",
      "  \"document_quality_scores\": {\n",
      "    \"readable\": 4,\n",
      "    \"accurate\": 3,\n",
      "    \"informative\": 3,\n",
      "    \"trustworthy\": 4\n",
      "  },\n",
      "  \"extractive\": [\n",
      "    1,\n",
      "    3,\n",
      "    4\n",
      "  ],\n",
      "  \"abstractive\": [\n",
      "    \"주주가치 포커스를 운용하는 KB자산운용이  SK와 알파벳(구글 지주회사)의 모범적 지배구조를 평가하면서 KMH에게는성공적 M&A로 순수익이 280% 늘었음에도 주가가 35%에 그친것은 복잡한 지배구조와 나빠진 현금흐름으로 분석하는주주서한을 보냈다.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Key structure of such document:\n",
      "['id', 'category', 'media_type', 'media_sub_type', 'media_name', 'size', 'char_count', 'publish_date', 'title', 'text', 'annotator_id', 'document_quality_scores', 'extractive', 'abstractive']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the sturcture of the train / val news text\n",
    "print(check_data_structure(news_train_file_path))\n",
    "print(check_data_structure(news_val_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore 계산을 위한 함수\n",
    "def compute_bert_score(text1, text2):\n",
    "    \"\"\"\n",
    "    두 텍스트 간의 BERTScore를 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): 첫 번째 텍스트\n",
    "        text2 (str): 두 번째 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        float: 두 텍스트 간의 BERTScore (F1 점수)\n",
    "    \"\"\"\n",
    "    \n",
    "    # BERTScore 계산 (한국어 모델 사용)\n",
    "    P, R, F1 = score([text1], [text2], lang=\"ko\")\n",
    "    \n",
    "    # F1 점수 반환 (일반적으로 가장 많이 사용되는 지표)\n",
    "    return F1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP을 통한 중요 토큰 식별 함수 (실제 구현 시 추가)\n",
    "def identify_important_tokens_with_shap(text, summary):\n",
    "    \"\"\"\n",
    "    SHAP을 사용하여 중요한 토큰을 식별합니다. (향후 구현 예정)\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원문 텍스트\n",
    "        summary (str): 요약 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        dict: {토큰: 중요도} 형태의 딕셔너리\n",
    "    \"\"\"\n",
    "    # 임시로 빈 함수 - 실제 SHAP 구현은 향후 추가 예정\n",
    "    # 임시 데이터 반환\n",
    "    tokens = text.split()[:10]  # 임시로 처음 10개 단어만 사용\n",
    "    importance_scores = np.random.random(len(tokens))\n",
    "    important_tokens = {tokens[i]: float(importance_scores[i]) for i in range(len(tokens))}\n",
    "    return important_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM을 통한 요약문 생성 함수 (실제 구현 시 추가)\n",
    "def generate_summary_with_llm(text, annotations=None):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 텍스트 요약을 생성합니다. (향후 구현 예정)\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원문 텍스트\n",
    "        annotations (dict, optional): 중요 토큰 및 주석\n",
    "    \n",
    "    Returns:\n",
    "        str: 생성된 요약문\n",
    "    \"\"\"\n",
    "    # 실제 구현 시 적절한 LLM API 사용\n",
    "    # 임시로 원문의 앞부분을 반환\n",
    "    if text:\n",
    "        shortened_text = ' '.join(text.split()[:30])\n",
    "        return f\"이것은 임시 요약입니다: {shortened_text}...\"\n",
    "    return \"요약 내용 없음\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트를 정제하는 함수: 기자 이름, 이메일, 불필요한 특수문자 등을 제거합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 정제할 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        str: 정제된 텍스트\n",
    "    \"\"\"\n",
    "    # 1. 이메일 주소 제거\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
    "    \n",
    "    # 2. '기자' 패턴 제거 (예: \"홍길동 기자\", \"김기자\")\n",
    "    text = re.sub(r'\\S+\\s+기자', '', text)\n",
    "    text = re.sub(r'\\S+기자', '', text)\n",
    "    \n",
    "    # 3. 특정 패턴 제거\n",
    "    patterns_to_remove = [\n",
    "        r'사진제공.+',           # '사진제공' 다음에 오는 텍스트\n",
    "        r'사진=.+?기자',         # '사진=' 패턴\n",
    "        r'자료제공.+',           # '자료제공' 다음에 오는 텍스트\n",
    "        r'\\(서울=.+?\\)',         # (서울=...) 패턴\n",
    "        r'\\([가-힣]{2,}=.+?\\)',  # (지역명=...) 패턴\n",
    "        r'\\[.+?\\]',              # [...] 대괄호 안의 내용\n",
    "        r'▲.+',                  # ▲ 다음에 오는 텍스트\n",
    "        r'■.+',                  # ■ 다음에 오는 텍스트\n",
    "        r'◆.+',                  # ◆ 다음에 오는 텍스트\n",
    "        r'【.+】',               # 【...】 패턴\n",
    "        r'『.+』',               # 『...』 패턴\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # 4. 기타 불필요한 특수문자 제거 (단, 인용문에 필요한 따옴표와 기본 구둣점은 유지)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\\"\\'\\?\\!\\:\\;]+', ' ', text)  # 기본 문장부호 외 특수문자 제거\n",
    "    \n",
    "    # 5. 연속된 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 6. 문장 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_dataset(input_path, output_path, limit=None):\n",
    "    \"\"\"\n",
    "    뉴스 데이터셋을 처리하여 새로운 JSON 구조로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): 입력 데이터셋 경로\n",
    "        output_path (str): 출력 JSON 파일 경로\n",
    "        limit (int, optional): 처리할 최대 문서 수 (테스트용)\n",
    "    \"\"\"\n",
    "    path = Path(input_path)\n",
    "    json_files = list(path.glob('*.json'))\n",
    "    print(f\"발견된 JSON 파일 수: {len(json_files)}\")\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"디렉토리에 JSON 파일이 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    processed_data = {\n",
    "        \"documents\": []\n",
    "    }\n",
    "    \n",
    "    doc_count = 0\n",
    "    \n",
    "    # 각 JSON 파일 처리\n",
    "    for file_idx, json_file in enumerate(tqdm(json_files, desc=\"파일 처리 중\")):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if 'documents' not in data:\n",
    "                    print(f\"{json_file.name}에서 'documents' 필드를 찾을 수 없습니다.\")\n",
    "                    continue\n",
    "                \n",
    "                # 경제 카테고리 문서만 필터링\n",
    "                econ_docs = [doc for doc in data['documents'] if doc.get('category') == '경제']\n",
    "                \n",
    "                for doc in tqdm(econ_docs, desc=f\"파일 {file_idx+1}/{len(json_files)} 문서 처리 중\", leave=False):\n",
    "                    # 제한된 수의 문서만 처리 (테스트용)\n",
    "                    if limit and doc_count >= limit:\n",
    "                        break\n",
    "                    \n",
    "                    # 원문 추출\n",
    "                    if 'text' not in doc:\n",
    "                        continue\n",
    "                    \n",
    "                    text_blocks = doc['text']\n",
    "                    original_text = \"\"\n",
    "                    \n",
    "                    # 문장 구성\n",
    "                    for block in text_blocks:\n",
    "                        for sentence in block:\n",
    "                            original_text += sentence['sentence'] + \" \"\n",
    "                    \n",
    "                    # 텍스트 정제\n",
    "                    # original_text = clean_text(original_text)\n",
    "                    \n",
    "                    # Ground Truth 요약문 (추상 요약)\n",
    "                    if 'abstractive' in doc and doc['abstractive']:\n",
    "                        ground_truth = doc['abstractive'][0] if isinstance(doc['abstractive'], list) else doc['abstractive']\n",
    "                    else:\n",
    "                        # 추상 요약이 없는 경우 건너뛰기\n",
    "                        continue\n",
    "                    \n",
    "                    # 이 부분들은 향후에 실제 구현 예정 (현재는 구조만)\n",
    "                    # 1. LLM을 사용한 첫 번째 요약문 생성 (원문만 사용)\n",
    "                    summary_without_annotations = generate_summary_with_llm(original_text)\n",
    "                    \n",
    "                    # 2. SHAP을 통한 중요 토큰 식별\n",
    "                    important_tokens = identify_important_tokens_with_shap(original_text, ground_truth)\n",
    "                    \n",
    "                    # 3. 주석 생성 (임시)\n",
    "                    annotations = {}\n",
    "                    for token, importance in important_tokens.items():\n",
    "                        annotations[token] = f\"이 단어는 요약에 중요합니다. 중요도: {importance:.4f}\"\n",
    "                    \n",
    "                    # 4. 두 번째 요약문 생성 (원문 + 주석 사용)\n",
    "                    summary_with_annotations = generate_summary_with_llm(original_text, annotations)\n",
    "                    \n",
    "                    # 5. BERTScore 계산 (임시)\n",
    "                    bert_score_original = compute_bert_score(original_text, ground_truth)\n",
    "                    bert_score_summary1 = compute_bert_score(summary_without_annotations, ground_truth)\n",
    "                    bert_score_summary2 = compute_bert_score(summary_with_annotations, ground_truth)\n",
    "                    \n",
    "                    # 새로운 구조의 문서 생성\n",
    "                    processed_doc = {\n",
    "                        \"id\": doc.get('id', ''),\n",
    "                        \"category\": doc.get('category', ''),\n",
    "                        \"title\": doc.get('title', ''),\n",
    "                        \"original_text\": original_text,\n",
    "                        \"annotations\": annotations,\n",
    "                        \"ground_truth_summary\": ground_truth,\n",
    "                        \"summary_without_annotations\": summary_without_annotations,\n",
    "                        \"summary_with_annotations\": summary_with_annotations,\n",
    "                        \"bert_score_original\": float(bert_score_original),\n",
    "                        \"bert_score_summary_without_annotations\": float(bert_score_summary1),\n",
    "                        \"bert_score_summary_with_annotations\": float(bert_score_summary2)\n",
    "                    }\n",
    "                    \n",
    "                    processed_data[\"documents\"].append(processed_doc)\n",
    "                    doc_count += 1\n",
    "                    \n",
    "                    # 제한된 수의 문서만 처리 (테스트용)\n",
    "                    if limit and doc_count >= limit:\n",
    "                        break\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"오류: {json_file.name}을 유효한 JSON으로 파싱할 수 없습니다.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"{json_file.name} 처리 중 오류 발생: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 제한된 수의 문서만 처리 (테스트용)\n",
    "        if limit and doc_count >= limit:\n",
    "            break\n",
    "    \n",
    "    # 처리된 데이터에 메타데이터 추가\n",
    "    processed_data[\"metadata\"] = {\n",
    "        \"total_documents\": len(processed_data[\"documents\"]),\n",
    "        \"source_files\": len(json_files),\n",
    "        \"creation_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"text_cleaning_applied\": True\n",
    "    }\n",
    "    \n",
    "    # 처리된 데이터 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"처리된 경제 뉴스 문서 수: {len(processed_data['documents'])}\")\n",
    "    print(f\"데이터셋이 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "news_train_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Training/신문기사_train_original\"\n",
    "news_val_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Validation/신문기사_valid_original\"\n",
    "\n",
    "output_train_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Training/processed_econ_news_train.json\"\n",
    "output_val_path = \"C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Validation/processed_econ_news_val.json\"\n",
    "\n",
    "test_limit = 10  # 테스트용으로 10개 문서만 처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 JSON 파일 수: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "파일 처리 중:   0%|          | 0/1 [00:42<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 경제 뉴스 문서 수: 10\n",
      "데이터셋이 C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Training/processed_econ_news_train.json에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 처리\n",
    "process_news_dataset(news_train_path, output_train_path, limit=test_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 JSON 파일 수: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "파일 처리 중:   0%|          | 0/1 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 경제 뉴스 문서 수: 10\n",
      "데이터셋이 C:/Users/99kih/OneDrive/바탕 화면/논문 스터디/Project/Data/Validation/processed_econ_news_val.json에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_news_dataset(news_val_path, output_val_path, limit=test_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
