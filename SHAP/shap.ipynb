{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 토크나이저 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/venv/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1682: FutureWarning: Loading a multimodal model with `AutoModelForCausalLM` is deprecated and will be removed in v5. `AutoModelForCausalLM` will be used to load only the text-to-text generation module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7874eeb171c34672acba0a8c4c806df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료 (device: cuda:0)\n",
      "임베딩 모델 로딩 중...\n",
      "임베딩 모델 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 및 토크나이저 로드 (양자화 없음)\n",
    "print(\"모델 및 토크나이저 로딩 중...\")\n",
    "model_name = \"google/gemma-3-12b-it\"  # Gemma 3 12B 모델 (instruction tuned)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 모델 로드 (양자화 없음)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"모델 로드 완료 (device: {device})\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU 로드 실패: {e}\")\n",
    "    print(\"CPU로 대체 모델 로드 중...\")\n",
    "    \n",
    "    # GPU 로드 실패 시 더 작은 대체 모델 CPU로 로드\n",
    "    fallback_model_name = \"google/gemma-3-3b-it\"  # 가장 작은 Gemma 3 모델\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        fallback_model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fallback_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"대체 모델 로드 완료 (device: {device}, model: {fallback_model_name})\")\n",
    "\n",
    "# sentence embedding 모델 로드 (다국어 지원 모델)\n",
    "print(\"임베딩 모델 로딩 중...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\").to(device)\n",
    "print(\"임베딩 모델 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 프롬프트 기반 text generation 함수\n",
    "def summarize_text(text):\n",
    "    \"\"\"텍스트를 요약하는 함수\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gemma 모델용 instruction 형식 프롬프트\n",
    "    prompt = f\"<start_of_turn>user\\n경제금융 뉴스 기사를 요약해주세요. 핵심 정보만 간결하게 요약해주세요.\\n\\n기사: {text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # 토크나이징 및 입력 준비\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 생성 설정\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id\n",
    "    }\n",
    "    \n",
    "    # 요약 생성\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, **gen_config)\n",
    "    \n",
    "    # 디코딩 및 프롬프트 제거\n",
    "    full_response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    # 응답 파싱 (모델 응답 부분만 추출)\n",
    "    try:\n",
    "        # Gemma 응답 형식에서 모델 응답 부분만 추출\n",
    "        if \"<start_of_turn>model\" in full_response:\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\\n\")[1]\n",
    "            if \"<end_of_turn>\" in response_parts:\n",
    "                summary = response_parts.split(\"<end_of_turn>\")[0].strip()\n",
    "            else:\n",
    "                summary = response_parts.strip()\n",
    "        else:\n",
    "            # 프롬프트 제거\n",
    "            summary = full_response.replace(prompt, \"\").strip()\n",
    "            \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"요약 파싱 오류: {e}\")\n",
    "        # 오류 발생 시 전체 응답에서 프롬프트 부분 제거 시도\n",
    "        return full_response.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터셋 가져오기\n",
    "def load_dataset(file_path, max_samples=50):\n",
    "    \"\"\"JSON 파일에서 데이터셋 로드\"\"\"\n",
    "    print(f\"데이터셋 로딩 중: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # JSON 구조에 따라 적절히 변환\n",
    "        if isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            # JSON 구조가 다르면 적절히 변환 필요\n",
    "            df = pd.DataFrame([data])\n",
    "        \n",
    "        # 컬럼 이름 확인 및 변환\n",
    "        if 'article' in df.columns and 'text' not in df.columns:\n",
    "            df['text'] = df['article']\n",
    "        if 'summary' not in df.columns and 'abstractive' in df.columns:\n",
    "            df['summary'] = df['abstractive']\n",
    "        \n",
    "        # 최대 샘플 수 제한\n",
    "        if len(df) > max_samples:\n",
    "            df = df.sample(max_samples, random_state=42)\n",
    "        \n",
    "        print(f\"데이터셋 로딩 완료: {len(df)} 샘플\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"데이터셋 로딩 실패: {e}\")\n",
    "        # 예시 데이터 생성\n",
    "        print(\"예시 데이터 사용\")\n",
    "        example_data = {\n",
    "            'text': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리를 동결했다. 한국은행은 지난해 4분기 이후 계속된 경기침체의 영향으로 고용 시장이 위축되고 있으며, 물가상승률은 목표 수준으로 안정되고 있다고 판단했다. 시장 전문가들은 한국 경제의 회복세가 예상보다 더디게 진행되고 있어 금리 인하 가능성도 있다고 전망했다.\",\n",
    "                \"금융위원회는 오늘 가계부채 관리 방안을 발표했다. 주요 내용은 총부채원리금상환비율(DSR) 규제를 40%로 강화하고, 다주택자에 대한 주택담보대출 제한을 확대하는 것이다. 또한 실수요자에 대한 대출 지원은 확대하되, 투기 목적의 대출에는 제재를 강화하는 투트랙 전략을 펼치기로 했다.\"\n",
    "            ],\n",
    "            'summary': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리 동결, 경기침체로 인한 고용시장 위축과 물가 안정 판단\",\n",
    "                \"금융위, 가계부채 관리방안 발표 - DSR 40% 강화, 다주택자 대출제한 확대, 실수요자 지원 확대\"\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAP용 파이프라인 및 Explainer 구성\n",
    "class SummarizationPipeline:\n",
    "    def __init__(self, model, tokenizer, embedder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.original_text = None\n",
    "        self.reference_summary = None\n",
    "        self.perturbation_count = 0  # 처리된 perturbation 수 추적\n",
    "    \n",
    "    def set_reference(self, text, summary=None):\n",
    "        \"\"\"원본 텍스트와 참조 요약 설정\"\"\"\n",
    "        self.original_text = text\n",
    "        self.perturbation_count = 0\n",
    "        \n",
    "        if summary is None:\n",
    "            # 참조 요약이 없으면 모델로 생성\n",
    "            print(\"참조 요약 생성 중...\")\n",
    "            self.reference_summary = summarize_text(text)\n",
    "            print(f\"생성된 참조 요약: {self.reference_summary}\")\n",
    "        else:\n",
    "            self.reference_summary = summary\n",
    "            print(f\"제공된 참조 요약: {self.reference_summary}\")\n",
    "        \n",
    "        # 참조 요약 임베딩 미리 계산\n",
    "        self.reference_embedding = self.embedder.encode(\n",
    "            self.reference_summary, convert_to_tensor=True\n",
    "        )\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        SHAP용 호출 함수. Perturbation된 텍스트 목록을 받아 각각의 요약 품질 점수 반환\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        batch_size = min(4, len(texts))  # 배치 처리 크기 (메모리 고려)\n",
    "        all_scores = []\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        self.perturbation_count += len(texts)\n",
    "        if self.perturbation_count % 10 == 0:\n",
    "            print(f\"SHAP 분석 진행 중: {self.perturbation_count}개 perturbation 처리됨\")\n",
    "            \n",
    "        # 배치 단위로 처리\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_scores = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                if not text or len(text.strip()) == 0:\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # 요약 생성\n",
    "                    summary = summarize_text(text)\n",
    "                    \n",
    "                    # 생성된 요약의 임베딩 계산\n",
    "                    summary_embedding = self.embedder.encode(\n",
    "                        summary, convert_to_tensor=True\n",
    "                    )\n",
    "                    \n",
    "                    # 참조 요약과의 유사도 계산 (코사인 유사도)\n",
    "                    similarity = util.cos_sim(\n",
    "                        summary_embedding, self.reference_embedding\n",
    "                    ).item()\n",
    "                    \n",
    "                    # ROUGE 점수 고려 (옵션)\n",
    "                    # rouge_score = calculate_rouge(summary, self.reference_summary)\n",
    "                    # combined_score = 0.7 * similarity + 0.3 * rouge_score\n",
    "                    \n",
    "                    batch_scores.append(float(similarity))\n",
    "                except Exception as e:\n",
    "                    print(f\"[!] 점수 계산 오류: {e}\")\n",
    "                    batch_scores.append(0.0)  # 오류 시 0점 처리\n",
    "            \n",
    "            all_scores.extend(batch_scores)\n",
    "        \n",
    "        return np.array(all_scores)\n",
    "    \n",
    "    def explain_specific_tokens(self, tokens, top_n=5):\n",
    "        \"\"\"\n",
    "        특정 토큰들의 중요도를 개별적으로 분석\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        original_summary = summarize_text(self.original_text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            # 해당 토큰을 제거한 텍스트 생성\n",
    "            removed_text = self.original_text.replace(token, \"\")\n",
    "            \n",
    "            # 제거 후 요약 생성\n",
    "            removed_summary = summarize_text(removed_text)\n",
    "            \n",
    "            # 원본 요약과 제거 후 요약의 유사도 계산\n",
    "            original_emb = self.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "            removed_emb = self.embedder.encode(removed_summary, convert_to_tensor=True)\n",
    "            \n",
    "            # 유사도 차이가 클수록 해당 토큰이 중요함\n",
    "            similarity = util.cos_sim(original_emb, removed_emb).item()\n",
    "            importance = 1.0 - similarity\n",
    "            \n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"importance\": importance,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"removed_summary\": removed_summary\n",
    "            })\n",
    "            \n",
    "        # 중요도 순으로 정렬\n",
    "        results.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "\n",
    "def analyze_with_shap(text, reference_summary=None, num_samples=50, verbose=True):\n",
    "    \"\"\"\n",
    "    단일 텍스트에 대해 SHAP 분석 수행\n",
    "    \n",
    "    Args:\n",
    "        text: 분석할 원문 텍스트\n",
    "        reference_summary: 참조 요약 (없으면 모델로 생성)\n",
    "        num_samples: SHAP 샘플링 수\n",
    "        verbose: 자세한 출력 여부\n",
    "    \n",
    "    Returns:\n",
    "        shap_values: SHAP 값\n",
    "        summary: 생성된 요약\n",
    "        pipeline: 분석에 사용된 파이프라인 객체\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\n원문 분석 시작\\n{'='*80}\")\n",
    "        print(f\"원문 (일부): {text[:200]}...\")\n",
    "    \n",
    "    # 입력 텍스트가 너무 길면 잘라내기 (Gemma 모델 컨텍스트 길이 제한 고려)\n",
    "    max_input_length = 4096  # Gemma 모델의 최대 입력 길이보다 작게 설정\n",
    "    if len(text) > max_input_length:\n",
    "        print(f\"⚠️ 입력 텍스트가 너무 깁니다. {max_input_length}자로 잘라냅니다.\")\n",
    "        text = text[:max_input_length]\n",
    "    \n",
    "    # 파이프라인 초기화 및 참조 설정\n",
    "    pipeline = SummarizationPipeline(model, tokenizer, embedder)\n",
    "    pipeline.set_reference(text, reference_summary)\n",
    "    \n",
    "    # SHAP Explainer 생성 (Text Masker 사용)\n",
    "    try:\n",
    "        # 토크나이저에 mask_token이 있는지 확인\n",
    "        mask_token = tokenizer.mask_token if hasattr(tokenizer, 'mask_token') and tokenizer.mask_token else \"[MASK]\"\n",
    "        \n",
    "        # Gemma 모델은 토크나이저 기반 마스킹 대신 단어 단위 마스킹 사용\n",
    "        partition_masker = shap.maskers.Text(\n",
    "            tokenizer=None,  # None으로 설정하면 단어 단위 분할 사용\n",
    "            mask_token=\"\",   # 빈 문자열로 마스킹\n",
    "            collapse_mask_token=True\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP Explainer 초기화 (마스크 토큰: {mask_token})\")\n",
    "        \n",
    "        # Explainer 생성 (auto 알고리즘 사용)\n",
    "        explainer = shap.Explainer(pipeline, partition_masker, algorithm=\"permutation\")\n",
    "        \n",
    "        # SHAP 값 계산\n",
    "        if verbose:\n",
    "            print(f\"SHAP 값 계산 중 (샘플 수: {num_samples})...\")\n",
    "        \n",
    "        # 배치 크기와 최대 평가 수 조정 (Gemma 모델 메모리 요구사항 고려)\n",
    "        shap_values = explainer(\n",
    "            [text], \n",
    "            max_evals=num_samples, \n",
    "            batch_size=1,\n",
    "            silent=not verbose\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"SHAP 분석 완료\")\n",
    "        \n",
    "        return shap_values, pipeline.reference_summary, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 중 오류 발생: {e}\")\n",
    "        print(\"대체 분석 방법 사용...\")\n",
    "        \n",
    "        # SHAP 실패 시 대체 분석 방법: 핵심 키워드 추출 및 중요도 직접 계산\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            \n",
    "            # TF-IDF로 핵심 키워드 추출\n",
    "            vectorizer = TfidfVectorizer(max_features=100)\n",
    "            tfidf_matrix = vectorizer.fit_transform([text])\n",
    "            \n",
    "            # 중요 키워드 및 점수 추출\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.toarray()[0]\n",
    "            \n",
    "            # Dummy SHAP 값 생성 (TF-IDF 기반)\n",
    "            dummy_values = np.zeros((1, len(text.split())))\n",
    "            dummy_data = np.array([text])\n",
    "            \n",
    "            # 기본 SHAP 결과 형식 모방\n",
    "            from collections import namedtuple\n",
    "            DummyShapValues = namedtuple('DummyShapValues', ['values', 'data', 'feature_names'])\n",
    "            \n",
    "            dummy_shap = DummyShapValues(\n",
    "                values=dummy_values,\n",
    "                data=dummy_data,\n",
    "                feature_names=['word_' + str(i) for i in range(len(text.split()))]\n",
    "            )\n",
    "            \n",
    "            # 핵심 키워드 목록 출력\n",
    "            print(\"\\n핵심 키워드 (TF-IDF 기반):\")\n",
    "            for word, score in sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                print(f\"  - {word}: {score:.4f}\")\n",
    "            \n",
    "            return dummy_shap, pipeline.reference_summary, pipeline\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"대체 분석도 실패: {e2}\")\n",
    "            return None, pipeline.reference_summary, pipeline\n",
    "\n",
    "\n",
    "def print_important_features(shap_values, summary, top_n=10):\n",
    "    \"\"\"\n",
    "    SHAP 값을 기반으로 중요 feature를 출력\n",
    "    \"\"\"\n",
    "    # SHAP 값의 절대값을 기준으로 정렬\n",
    "    token_importances = []\n",
    "    \n",
    "    for i, token in enumerate(shap_values.data[0].split()):\n",
    "        importance = abs(shap_values.values[0][i])\n",
    "        token_importances.append((token, importance, shap_values.values[0][i]))\n",
    "    \n",
    "    # 중요도 기준 정렬\n",
    "    sorted_importances = sorted(token_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"생성된 요약: {summary}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"상위 {top_n}개 중요 단어 (SHAP 기준):\")\n",
    "    print(f\"{'단어':<15} | {'중요도':>10} | {'영향':>10} | {'해석'}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    for token, importance, raw_value in sorted_importances[:top_n]:\n",
    "        effect = \"긍정적 영향\" if raw_value > 0 else \"부정적 영향\"\n",
    "        print(f\"{token:<15} | {importance:>10.4f} | {raw_value:>10.4f} | {effect}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def save_shap_visualization(shap_values, output_file=\"shap_summary_analysis.png\"):\n",
    "    \"\"\"SHAP 시각화 저장\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.plots.text(shap_values, display=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"SHAP 시각화 저장 완료: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로딩 중: train_original.json\n",
      "데이터셋 로딩 실패: Expecting value: line 9032102 column 19 (char 226168964)\n",
      "예시 데이터 사용\n",
      "\n",
      "총 2개 샘플 분석 시작\n",
      "\n",
      "샘플 1/2 분석:\n",
      "\n",
      "================================================================================\n",
      "원문 분석 시작\n",
      "================================================================================\n",
      "원문 (일부): 한국은행이 통화정책 결정 회의에서 기준금리를 동결했다. 한국은행은 지난해 4분기 이후 계속된 경기침체의 영향으로 고용 시장이 위축되고 있으며, 물가상승률은 목표 수준으로 안정되고 있다고 판단했다. 시장 전문가들은 한국 경제의 회복세가 예상보다 더디게 진행되고 있어 금리 인하 가능성도 있다고 전망했다....\n",
      "제공된 참조 요약: 한국은행이 통화정책 결정 회의에서 기준금리 동결, 경기침체로 인한 고용시장 위축과 물가 안정 판단\n",
      "SHAP 분석 중 오류 발생: list index out of range\n",
      "대체 분석 방법 사용...\n",
      "\n",
      "핵심 키워드 (TF-IDF 기반):\n",
      "  - 있다고: 0.3203\n",
      "  - 4분기: 0.1601\n",
      "  - 가능성도: 0.1601\n",
      "  - 결정: 0.1601\n",
      "  - 경기침체의: 0.1601\n",
      "  - 경제의: 0.1601\n",
      "  - 계속된: 0.1601\n",
      "  - 고용: 0.1601\n",
      "  - 금리: 0.1601\n",
      "  - 기준금리를: 0.1601\n",
      "\n",
      "================================================================================\n",
      "생성된 요약: 한국은행이 통화정책 결정 회의에서 기준금리 동결, 경기침체로 인한 고용시장 위축과 물가 안정 판단\n",
      "================================================================================\n",
      "상위 15개 중요 단어 (SHAP 기준):\n",
      "단어              |        중요도 |         영향 | 해석\n",
      "------------------------------------------------------------\n",
      "한국은행이           |     0.0000 |     0.0000 | 부정적 영향\n",
      "통화정책            |     0.0000 |     0.0000 | 부정적 영향\n",
      "결정              |     0.0000 |     0.0000 | 부정적 영향\n",
      "회의에서            |     0.0000 |     0.0000 | 부정적 영향\n",
      "기준금리를           |     0.0000 |     0.0000 | 부정적 영향\n",
      "동결했다.           |     0.0000 |     0.0000 | 부정적 영향\n",
      "한국은행은           |     0.0000 |     0.0000 | 부정적 영향\n",
      "지난해             |     0.0000 |     0.0000 | 부정적 영향\n",
      "4분기             |     0.0000 |     0.0000 | 부정적 영향\n",
      "이후              |     0.0000 |     0.0000 | 부정적 영향\n",
      "계속된             |     0.0000 |     0.0000 | 부정적 영향\n",
      "경기침체의           |     0.0000 |     0.0000 | 부정적 영향\n",
      "영향으로            |     0.0000 |     0.0000 | 부정적 영향\n",
      "고용              |     0.0000 |     0.0000 | 부정적 영향\n",
      "시장이             |     0.0000 |     0.0000 | 부정적 영향\n",
      "================================================================================\n",
      "SHAP 시각화를 생성할 수 없습니다 - 대체 분석 방법 사용됨\n",
      "\n",
      "샘플 2/2 분석:\n",
      "\n",
      "================================================================================\n",
      "원문 분석 시작\n",
      "================================================================================\n",
      "원문 (일부): 금융위원회는 오늘 가계부채 관리 방안을 발표했다. 주요 내용은 총부채원리금상환비율(DSR) 규제를 40%로 강화하고, 다주택자에 대한 주택담보대출 제한을 확대하는 것이다. 또한 실수요자에 대한 대출 지원은 확대하되, 투기 목적의 대출에는 제재를 강화하는 투트랙 전략을 펼치기로 했다....\n",
      "제공된 참조 요약: 금융위, 가계부채 관리방안 발표 - DSR 40% 강화, 다주택자 대출제한 확대, 실수요자 지원 확대\n",
      "SHAP 분석 중 오류 발생: list index out of range\n",
      "대체 분석 방법 사용...\n",
      "\n",
      "핵심 키워드 (TF-IDF 기반):\n",
      "  - 대한: 0.3333\n",
      "  - 40: 0.1667\n",
      "  - dsr: 0.1667\n",
      "  - 가계부채: 0.1667\n",
      "  - 강화하고: 0.1667\n",
      "  - 강화하는: 0.1667\n",
      "  - 것이다: 0.1667\n",
      "  - 관리: 0.1667\n",
      "  - 규제를: 0.1667\n",
      "  - 금융위원회는: 0.1667\n",
      "\n",
      "================================================================================\n",
      "생성된 요약: 금융위, 가계부채 관리방안 발표 - DSR 40% 강화, 다주택자 대출제한 확대, 실수요자 지원 확대\n",
      "================================================================================\n",
      "상위 15개 중요 단어 (SHAP 기준):\n",
      "단어              |        중요도 |         영향 | 해석\n",
      "------------------------------------------------------------\n",
      "금융위원회는          |     0.0000 |     0.0000 | 부정적 영향\n",
      "오늘              |     0.0000 |     0.0000 | 부정적 영향\n",
      "가계부채            |     0.0000 |     0.0000 | 부정적 영향\n",
      "관리              |     0.0000 |     0.0000 | 부정적 영향\n",
      "방안을             |     0.0000 |     0.0000 | 부정적 영향\n",
      "발표했다.           |     0.0000 |     0.0000 | 부정적 영향\n",
      "주요              |     0.0000 |     0.0000 | 부정적 영향\n",
      "내용은             |     0.0000 |     0.0000 | 부정적 영향\n",
      "총부채원리금상환비율(DSR) |     0.0000 |     0.0000 | 부정적 영향\n",
      "규제를             |     0.0000 |     0.0000 | 부정적 영향\n",
      "40%로            |     0.0000 |     0.0000 | 부정적 영향\n",
      "강화하고,           |     0.0000 |     0.0000 | 부정적 영향\n",
      "다주택자에           |     0.0000 |     0.0000 | 부정적 영향\n",
      "대한              |     0.0000 |     0.0000 | 부정적 영향\n",
      "주택담보대출          |     0.0000 |     0.0000 | 부정적 영향\n",
      "================================================================================\n",
      "SHAP 시각화를 생성할 수 없습니다 - 대체 분석 방법 사용됨\n",
      "\n",
      "모든 분석 완료!\n"
     ]
    }
   ],
   "source": [
    "# 5. 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터셋 로드\n",
    "    dataset_path = \"train_original.json\"  # 실제 파일 경로로 변경\n",
    "    df = load_dataset(dataset_path, max_samples=10)  # 테스트용 10개만\n",
    "    \n",
    "    # 텍스트와 참조 요약 추출\n",
    "    texts = df[\"text\"].tolist()\n",
    "    references = df[\"summary\"].tolist() if \"summary\" in df.columns else [None] * len(texts)\n",
    "    \n",
    "    print(f\"\\n총 {len(texts)}개 샘플 분석 시작\")\n",
    "    \n",
    "    # 각 샘플에 대해 SHAP 분석 수행\n",
    "    for i, (text, reference) in enumerate(zip(texts, references)):\n",
    "        print(f\"\\n샘플 {i+1}/{len(texts)} 분석:\")\n",
    "        \n",
    "        try:\n",
    "            shap_values, summary, _ = analyze_with_shap(\n",
    "                text, \n",
    "                reference_summary=reference,\n",
    "                num_samples=100,  # 샘플링 수 조정 (높을수록 정확하지만 느림)\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # 중요 feature 출력\n",
    "            print_important_features(shap_values, summary, top_n=15)\n",
    "            \n",
    "            # SHAP 시각화 저장 - 객체 타입 확인 후 실행\n",
    "            # 'DummyShapValues'인 경우 시각화 건너뛰기\n",
    "            if not hasattr(shap_values, '__class__') or shap_values.__class__.__name__ != 'DummyShapValues':\n",
    "                save_shap_visualization(shap_values, f\"shap_analysis_sample_{i+1}.png\")\n",
    "            else:\n",
    "                print(\"SHAP 시각화를 생성할 수 없습니다 - 대체 분석 방법 사용됨\")\n",
    "        except Exception as e:\n",
    "            print(f\"샘플 {i+1} 분석 중 오류 발생: {e}\")\n",
    "            print(\"이 샘플은 건너뜁니다.\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n모든 분석 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
