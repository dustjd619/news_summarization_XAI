{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 토크나이저 로딩 중...\n",
      "GPU 로드 실패: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
      "CPU로 대체 모델 로드 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a547c9dc54482a9680d5a5dc74d8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434fcb3f25d4964b10cc34484c0f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f29c3981aeb48b7ad2526e1f2830e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc1270794fb4c7cbaaa056e9ff48ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935006ecf50f4c5f943f7b357b4ddfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55305070b51f4fd2828d514ff36488cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a858e322ec74e7d92609a1828dec986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3917f9651243f2ae65e73091dc460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cf60a56257437ab6dd4fcfc1aea7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb8716a31fa4bd28b656e5f52fd9305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34443c0d62d64654b001d9ecd8bd4ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9916fad8f410685897ce576d8be06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5ee045015c45ea9f37715ffe1e868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 모델 로드 완료 (device: cpu, model: google/gemma-2-9b-it)\n",
      "임베딩 모델 로딩 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61406dabd49e487bbd279bef9bd66233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b847705ab13e4cf3bc95f4af8580c403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4057cd669264450a548575fa1002a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae65178d8946e398ef854091e30907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3a23e70acf482aa22b538cf1a24037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ff37b0d9e4a0ca2a9a13876b06169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d21b3c06994745965262715dbbe951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a83db8ac7a4cd9ad6adaa87cfe9a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84fc536bc544535aac86f1f81ed48c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebadc63f71eb4a66a079d5cbe5677dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 모델 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 및 토크나이저 로드 (INT8 양자화 사용)\n",
    "print(\"모델 및 토크나이저 로딩 중...\")\n",
    "model_name = \"google/gemma-3-27b-it\"  # Gemma 3 27B 모델 (instruction tuned)\n",
    "\n",
    "# 8비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, \n",
    "    llm_int8_threshold=6.0, \n",
    "    llm_int8_has_fp16_weight=False\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 모델 로드 (양자화 적용)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # 자동으로 GPU 메모리 관리\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"모델 로드 완료 (device: {device})\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU 로드 실패: {e}\")\n",
    "    print(\"CPU로 대체 모델 로드 중...\")\n",
    "    \n",
    "    # GPU 로드 실패 시 더 작은 대체 모델 CPU로 로드\n",
    "    fallback_model_name = \"google/gemma-2-9b-it\"  # 더 작은 대체 모델\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        fallback_model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fallback_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"대체 모델 로드 완료 (device: {device}, model: {fallback_model_name})\")\n",
    "\n",
    "# sentence embedding 모델 로드 (다국어 지원 모델)\n",
    "print(\"임베딩 모델 로딩 중...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\").to(device)\n",
    "print(\"임베딩 모델 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 프롬프트 기반 text generation 함수\n",
    "def summarize_text(text):\n",
    "    \"\"\"텍스트를 요약하는 함수\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gemma 모델용 instruction 형식 프롬프트\n",
    "    prompt = f\"<start_of_turn>user\\n경제금융 뉴스 기사를 요약해주세요. 핵심 정보만 간결하게 요약해주세요.\\n\\n기사: {text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # 토크나이징 및 입력 준비\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 생성 설정\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id\n",
    "    }\n",
    "    \n",
    "    # 요약 생성\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, **gen_config)\n",
    "    \n",
    "    # 디코딩 및 프롬프트 제거\n",
    "    full_response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    # 응답 파싱 (모델 응답 부분만 추출)\n",
    "    try:\n",
    "        # Gemma 응답 형식에서 모델 응답 부분만 추출\n",
    "        if \"<start_of_turn>model\" in full_response:\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\\n\")[1]\n",
    "            if \"<end_of_turn>\" in response_parts:\n",
    "                summary = response_parts.split(\"<end_of_turn>\")[0].strip()\n",
    "            else:\n",
    "                summary = response_parts.strip()\n",
    "        else:\n",
    "            # 프롬프트 제거\n",
    "            summary = full_response.replace(prompt, \"\").strip()\n",
    "            \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"요약 파싱 오류: {e}\")\n",
    "        # 오류 발생 시 전체 응답에서 프롬프트 부분 제거 시도\n",
    "        return full_response.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터셋 가져오기\n",
    "def load_dataset(file_path, max_samples=50):\n",
    "    \"\"\"JSON 파일에서 데이터셋 로드\"\"\"\n",
    "    print(f\"데이터셋 로딩 중: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # JSON 구조에 따라 적절히 변환\n",
    "        if isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            # JSON 구조가 다르면 적절히 변환 필요\n",
    "            df = pd.DataFrame([data])\n",
    "        \n",
    "        # 컬럼 이름 확인 및 변환\n",
    "        if 'article' in df.columns and 'text' not in df.columns:\n",
    "            df['text'] = df['article']\n",
    "        if 'summary' not in df.columns and 'abstractive' in df.columns:\n",
    "            df['summary'] = df['abstractive']\n",
    "        \n",
    "        # 최대 샘플 수 제한\n",
    "        if len(df) > max_samples:\n",
    "            df = df.sample(max_samples, random_state=42)\n",
    "        \n",
    "        print(f\"데이터셋 로딩 완료: {len(df)} 샘플\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"데이터셋 로딩 실패: {e}\")\n",
    "        # 예시 데이터 생성\n",
    "        print(\"예시 데이터 사용\")\n",
    "        example_data = {\n",
    "            'text': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리를 동결했다. 한국은행은 지난해 4분기 이후 계속된 경기침체의 영향으로 고용 시장이 위축되고 있으며, 물가상승률은 목표 수준으로 안정되고 있다고 판단했다. 시장 전문가들은 한국 경제의 회복세가 예상보다 더디게 진행되고 있어 금리 인하 가능성도 있다고 전망했다.\",\n",
    "                \"금융위원회는 오늘 가계부채 관리 방안을 발표했다. 주요 내용은 총부채원리금상환비율(DSR) 규제를 40%로 강화하고, 다주택자에 대한 주택담보대출 제한을 확대하는 것이다. 또한 실수요자에 대한 대출 지원은 확대하되, 투기 목적의 대출에는 제재를 강화하는 투트랙 전략을 펼치기로 했다.\"\n",
    "            ],\n",
    "            'summary': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리 동결, 경기침체로 인한 고용시장 위축과 물가 안정 판단\",\n",
    "                \"금융위, 가계부채 관리방안 발표 - DSR 40% 강화, 다주택자 대출제한 확대, 실수요자 지원 확대\"\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 4. SHAP용 파이프라인 및 Explainer 구성\n",
    "class SummarizationPipeline:\n",
    "    def __init__(self, model, tokenizer, embedder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.original_text = None\n",
    "        self.reference_summary = None\n",
    "        self.perturbation_count = 0  # 처리된 perturbation 수 추적\n",
    "    \n",
    "    def set_reference(self, text, summary=None):\n",
    "        \"\"\"원본 텍스트와 참조 요약 설정\"\"\"\n",
    "        self.original_text = text\n",
    "        self.perturbation_count = 0\n",
    "        \n",
    "        if summary is None:\n",
    "            # 참조 요약이 없으면 모델로 생성\n",
    "            print(\"참조 요약 생성 중...\")\n",
    "            self.reference_summary = summarize_text(text)\n",
    "            print(f\"생성된 참조 요약: {self.reference_summary}\")\n",
    "        else:\n",
    "            self.reference_summary = summary\n",
    "            print(f\"제공된 참조 요약: {self.reference_summary}\")\n",
    "        \n",
    "        # 참조 요약 임베딩 미리 계산\n",
    "        self.reference_embedding = self.embedder.encode(\n",
    "            self.reference_summary, convert_to_tensor=True\n",
    "        )\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        SHAP용 호출 함수. Perturbation된 텍스트 목록을 받아 각각의 요약 품질 점수 반환\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        batch_size = min(4, len(texts))  # 배치 처리 크기 (메모리 고려)\n",
    "        all_scores = []\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        self.perturbation_count += len(texts)\n",
    "        if self.perturbation_count % 10 == 0:\n",
    "            print(f\"SHAP 분석 진행 중: {self.perturbation_count}개 perturbation 처리됨\")\n",
    "            \n",
    "        # 배치 단위로 처리\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_scores = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                if not text or len(text.strip()) == 0:\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # 요약 생성\n",
    "                    summary = summarize_text(text)\n",
    "                    \n",
    "                    # 생성된 요약의 임베딩 계산\n",
    "                    summary_embedding = self.embedder.encode(\n",
    "                        summary, convert_to_tensor=True\n",
    "                    )\n",
    "                    \n",
    "                    # 참조 요약과의 유사도 계산 (코사인 유사도)\n",
    "                    similarity = util.cos_sim(\n",
    "                        summary_embedding, self.reference_embedding\n",
    "                    ).item()\n",
    "                    \n",
    "                    # ROUGE 점수 고려 (옵션)\n",
    "                    # rouge_score = calculate_rouge(summary, self.reference_summary)\n",
    "                    # combined_score = 0.7 * similarity + 0.3 * rouge_score\n",
    "                    \n",
    "                    batch_scores.append(float(similarity))\n",
    "                except Exception as e:\n",
    "                    print(f\"[!] 점수 계산 오류: {e}\")\n",
    "                    batch_scores.append(0.0)  # 오류 시 0점 처리\n",
    "            \n",
    "            all_scores.extend(batch_scores)\n",
    "        \n",
    "        return np.array(all_scores)\n",
    "    \n",
    "    def explain_specific_tokens(self, tokens, top_n=5):\n",
    "        \"\"\"\n",
    "        특정 토큰들의 중요도를 개별적으로 분석\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        original_summary = summarize_text(self.original_text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            # 해당 토큰을 제거한 텍스트 생성\n",
    "            removed_text = self.original_text.replace(token, \"\")\n",
    "            \n",
    "            # 제거 후 요약 생성\n",
    "            removed_summary = summarize_text(removed_text)\n",
    "            \n",
    "            # 원본 요약과 제거 후 요약의 유사도 계산\n",
    "            original_emb = self.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "            removed_emb = self.embedder.encode(removed_summary, convert_to_tensor=True)\n",
    "            \n",
    "            # 유사도 차이가 클수록 해당 토큰이 중요함\n",
    "            similarity = util.cos_sim(original_emb, removed_emb).item()\n",
    "            importance = 1.0 - similarity\n",
    "            \n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"importance\": importance,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"removed_summary\": removed_summary\n",
    "            })\n",
    "            \n",
    "        # 중요도 순으로 정렬\n",
    "        results.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "\n",
    "def analyze_with_shap(text, reference_summary=None, num_samples=50, verbose=True):\n",
    "    \"\"\"\n",
    "    단일 텍스트에 대해 SHAP 분석 수행\n",
    "    \n",
    "    Args:\n",
    "        text: 분석할 원문 텍스트\n",
    "        reference_summary: 참조 요약 (없으면 모델로 생성)\n",
    "        num_samples: SHAP 샘플링 수\n",
    "        verbose: 자세한 출력 여부\n",
    "    \n",
    "    Returns:\n",
    "        shap_values: SHAP 값\n",
    "        summary: 생성된 요약\n",
    "        pipeline: 분석에 사용된 파이프라인 객체\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\n원문 분석 시작\\n{'='*80}\")\n",
    "        print(f\"원문 (일부): {text[:200]}...\")\n",
    "    \n",
    "    # 입력 텍스트가 너무 길면 잘라내기 (Gemma 모델 컨텍스트 길이 제한 고려)\n",
    "    max_input_length = 4096  # Gemma 모델의 최대 입력 길이보다 작게 설정\n",
    "    if len(text) > max_input_length:\n",
    "        print(f\"⚠️ 입력 텍스트가 너무 깁니다. {max_input_length}자로 잘라냅니다.\")\n",
    "        text = text[:max_input_length]\n",
    "    \n",
    "    # 파이프라인 초기화 및 참조 설정\n",
    "    pipeline = SummarizationPipeline(model, tokenizer, embedder)\n",
    "    pipeline.set_reference(text, reference_summary)\n",
    "    \n",
    "    # SHAP Explainer 생성 (Text Masker 사용)\n",
    "    try:\n",
    "        # 토크나이저에 mask_token이 있는지 확인\n",
    "        mask_token = tokenizer.mask_token if hasattr(tokenizer, 'mask_token') and tokenizer.mask_token else \"[MASK]\"\n",
    "        \n",
    "        # Gemma 모델은 토크나이저 기반 마스킹 대신 단어 단위 마스킹 사용\n",
    "        partition_masker = shap.maskers.Text(\n",
    "            tokenizer=None,  # None으로 설정하면 단어 단위 분할 사용\n",
    "            mask_token=\"\",   # 빈 문자열로 마스킹\n",
    "            collapse_mask_token=True\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP Explainer 초기화 (마스크 토큰: {mask_token})\")\n",
    "        \n",
    "        # Explainer 생성 (auto 알고리즘 사용)\n",
    "        explainer = shap.Explainer(pipeline, partition_masker, algorithm=\"permutation\")\n",
    "        \n",
    "        # SHAP 값 계산\n",
    "        if verbose:\n",
    "            print(f\"SHAP 값 계산 중 (샘플 수: {num_samples})...\")\n",
    "        \n",
    "        # 배치 크기와 최대 평가 수 조정 (Gemma 모델 메모리 요구사항 고려)\n",
    "        shap_values = explainer(\n",
    "            [text], \n",
    "            max_evals=num_samples, \n",
    "            batch_size=1,\n",
    "            silent=not verbose\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"SHAP 분석 완료\")\n",
    "        \n",
    "        return shap_values, pipeline.reference_summary, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 중 오류 발생: {e}\")\n",
    "        print(\"대체 분석 방법 사용...\")\n",
    "        \n",
    "        # SHAP 실패 시 대체 분석 방법: 핵심 키워드 추출 및 중요도 직접 계산\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            \n",
    "            # TF-IDF로 핵심 키워드 추출\n",
    "            vectorizer = TfidfVectorizer(max_features=100)\n",
    "            tfidf_matrix = vectorizer.fit_transform([text])\n",
    "            \n",
    "            # 중요 키워드 및 점수 추출\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.toarray()[0]\n",
    "            \n",
    "            # Dummy SHAP 값 생성 (TF-IDF 기반)\n",
    "            dummy_values = np.zeros((1, len(text.split())))\n",
    "            dummy_data = np.array([text])\n",
    "            \n",
    "            # 기본 SHAP 결과 형식 모방\n",
    "            from collections import namedtuple\n",
    "            DummyShapValues = namedtuple('DummyShapValues', ['values', 'data', 'feature_names'])\n",
    "            \n",
    "            dummy_shap = DummyShapValues(\n",
    "                values=dummy_values,\n",
    "                data=dummy_data,\n",
    "                feature_names=['word_' + str(i) for i in range(len(text.split()))]\n",
    "            )\n",
    "            \n",
    "            # 핵심 키워드 목록 출력\n",
    "            print(\"\\n핵심 키워드 (TF-IDF 기반):\")\n",
    "            for word, score in sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                print(f\"  - {word}: {score:.4f}\")\n",
    "            \n",
    "            return dummy_shap, pipeline.reference_summary, pipeline\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"대체 분석도 실패: {e2}\")\n",
    "            return None, pipeline.reference_summary, pipeline\n",
    "\n",
    "\n",
    "def print_important_features(shap_values, summary, top_n=10):\n",
    "    \"\"\"\n",
    "    SHAP 값을 기반으로 중요 feature를 출력\n",
    "    \"\"\"\n",
    "    # SHAP 값의 절대값을 기준으로 정렬\n",
    "    token_importances = []\n",
    "    \n",
    "    for i, token in enumerate(shap_values.data[0].split()):\n",
    "        importance = abs(shap_values.values[0][i])\n",
    "        token_importances.append((token, importance, shap_values.values[0][i]))\n",
    "    \n",
    "    # 중요도 기준 정렬\n",
    "    sorted_importances = sorted(token_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"생성된 요약: {summary}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"상위 {top_n}개 중요 단어 (SHAP 기준):\")\n",
    "    print(f\"{'단어':<15} | {'중요도':>10} | {'영향':>10} | {'해석'}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    for token, importance, raw_value in sorted_importances[:top_n]:\n",
    "        effect = \"긍정적 영향\" if raw_value > 0 else \"부정적 영향\"\n",
    "        print(f\"{token:<15} | {importance:>10.4f} | {raw_value:>10.4f} | {effect}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def save_shap_visualization(shap_values, output_file=\"shap_summary_analysis.png\"):\n",
    "    \"\"\"SHAP 시각화 저장\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.plots.text(shap_values, display=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"SHAP 시각화 저장 완료: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로딩 중: train_original.json\n"
     ]
    }
   ],
   "source": [
    "# 5. 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터셋 로드\n",
    "    dataset_path = \"train_original.json\"  # 실제 파일 경로로 변경\n",
    "    df = load_dataset(dataset_path, max_samples=10)  # 테스트용 10개만\n",
    "    \n",
    "    # 텍스트와 참조 요약 추출\n",
    "    texts = df[\"text\"].tolist()\n",
    "    references = df[\"summary\"].tolist() if \"summary\" in df.columns else [None] * len(texts)\n",
    "    \n",
    "    print(f\"\\n총 {len(texts)}개 샘플 분석 시작\")\n",
    "    \n",
    "    # 각 샘플에 대해 SHAP 분석 수행\n",
    "    for i, (text, reference) in enumerate(zip(texts, references)):\n",
    "        print(f\"\\n샘플 {i+1}/{len(texts)} 분석:\")\n",
    "        \n",
    "        shap_values, summary = analyze_with_shap(\n",
    "            text, \n",
    "            reference_summary=reference,\n",
    "            num_samples=100,  # 샘플링 수 조정 (높을수록 정확하지만 느림)\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 중요 feature 출력\n",
    "        print_important_features(shap_values, summary, top_n=15)\n",
    "        \n",
    "        # SHAP 시각화 저장 (선택사항)\n",
    "        save_shap_visualization(shap_values, f\"shap_analysis_sample_{i+1}.png\")\n",
    "    \n",
    "    print(\"\\n모든 분석 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news_xai_env)",
   "language": "python",
   "name": "news_xai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
