{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8028de4c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#debuggingìš© ì„¤ì •\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\n",
      "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ GPU: 1ê°œ\n",
      "í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.00 MB\n",
      "27B ëª¨ë¸ ë¡œë“œ ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15edad3f2624e8f8f8356fc3900828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: cuda:0)\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "model_name = \"google/gemma-3-27b-it\"  # Gemma 3 27B ëª¨ë¸ (instruction tuned)\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# CUDA ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ\")\n",
    "    print(f\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # íŒ¨ë”© í† í° ì„¤ì •\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ì‹œë„\n",
    "try:\n",
    "    print(f\"27B ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # ìë™ìœ¼ë¡œ ì ì ˆí•œ ì¥ì¹˜ì— í• ë‹¹\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"CPUë¡œ ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # CPUë¡œ ë‹¤ì‹œ ì‹œë„\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})\")\n",
    "\n",
    "# sentence embedding ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸)\n",
    "print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ text generation í•¨ìˆ˜\n",
    "def summarize_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gemma ëª¨ë¸ìš© instruction í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
    "    prompt = f\"<start_of_turn>user\\nê²½ì œê¸ˆìœµ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ë‹¨ì–´ ë° ë‚´ìš©ì¸ ë¬´ì—‡ì¸ì§€ ê³ ë ¤í•´ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”.\\n\\nê¸°ì‚¬: {text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§• ë° ì…ë ¥ ì¤€ë¹„\n",
    "    max_input_length = 2048-128\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"[DEBUG] input_ids shape:\", inputs[\"input_ids\"].shape)\n",
    "    print(\"[DEBUG] input_ids (ì• 30ê°œ):\", inputs[\"input_ids\"][0][:30]) \n",
    "    \n",
    "    # ìƒì„± ì„¤ì •\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "    \n",
    "    # ìš”ì•½ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\">>> input shape:\", inputs[\"input_ids\"].shape)\n",
    "\n",
    "            output = model.generate(**inputs, **gen_config)  # GPUì—ì„œ ìƒì„±\n",
    "            if output.dim() > 1:\n",
    "                output = output[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš© (batch_size=1)\n",
    "\n",
    "            if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "                print(\"[!] generate() ê²°ê³¼ì— nan ë˜ëŠ” inf í¬í•¨ë¨ â€” ë””ì½”ë”© ì¤‘ë‹¨\")\n",
    "                return \"\"\n",
    "\n",
    "            output = output.to(\"cpu\")  # CPUë¡œ ì˜®ê²¨ì„œ ì•ˆì „í•˜ê²Œ ë””ì½”ë”©\n",
    "            full_response = tokenizer.decode(output, skip_special_tokens=False)  # âœ… ì—¬ê¸° ìˆ˜ì •!\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] generate() ì˜¤ë¥˜: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    # ì‘ë‹µ íŒŒì‹± (ëª¨ë¸ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ)\n",
    "    try:\n",
    "        # Gemma ì‘ë‹µ í˜•ì‹ì—ì„œ ëª¨ë¸ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "        if \"<start_of_turn>model\" in full_response:\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\\n\")[1]\n",
    "            if \"<end_of_turn>\" in response_parts:\n",
    "                summary = response_parts.split(\"<end_of_turn>\")[0].strip()\n",
    "            else:\n",
    "                summary = response_parts.strip()\n",
    "        else:\n",
    "            # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "            summary = full_response.replace(prompt, \"\").strip()\n",
    "            \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"ìš”ì•½ íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° ì‹œë„\n",
    "        return full_response.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°\n",
    "def load_dataset(file_path, max_samples=50):\n",
    "    \"\"\"JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë¬¸ì„œì™€ ìš”ì•½ ë°ì´í„°ì…‹ ë¡œë“œ\"\"\"\n",
    "    print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
    "        if \"documents\" in data:\n",
    "            # ë¬¸ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "            documents = data[\"documents\"]\n",
    "            processed_data = []\n",
    "            \n",
    "            for doc in documents:\n",
    "                # ì›ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (textëŠ” ë°°ì—´ êµ¬ì¡°)\n",
    "                original_text = \"\"\n",
    "                if \"text\" in doc:\n",
    "                    # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°\n",
    "                    for paragraph in doc[\"text\"]:\n",
    "                        for sent_obj in paragraph:\n",
    "                            if \"sentence\" in sent_obj:\n",
    "                                original_text += sent_obj[\"sentence\"] + \" \"\n",
    "                \n",
    "                # ìš”ì•½ë¬¸ ì¶”ì¶œ\n",
    "                summary = \"\"\n",
    "                if \"abstractive\" in doc and doc[\"abstractive\"]:\n",
    "                    summary = doc[\"abstractive\"][0] if isinstance(doc[\"abstractive\"], list) else doc[\"abstractive\"]\n",
    "                \n",
    "                # í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìš© ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "                processed_doc = {\n",
    "                    \"id\": doc.get(\"id\", \"\"),\n",
    "                    \"title\": doc.get(\"title\", \"\"),\n",
    "                    \"text\": original_text.strip(),\n",
    "                    \"summary\": summary,\n",
    "                    \"category\": doc.get(\"category\", \"\"),\n",
    "                    \"media_name\": doc.get(\"media_name\", \"\")\n",
    "                }\n",
    "                \n",
    "                processed_data.append(processed_doc)\n",
    "            \n",
    "            # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "            df = pd.DataFrame(processed_data)\n",
    "            \n",
    "        else:\n",
    "            # ë‹¤ë¥¸ êµ¬ì¡°ì˜ JSON ì²˜ë¦¬\n",
    "            print(\"í‘œì¤€ êµ¬ì¡°ê°€ ì•„ë‹Œ JSON íŒŒì¼ì…ë‹ˆë‹¤.\")\n",
    "            if isinstance(data, list):\n",
    "                df = pd.DataFrame(data)\n",
    "            else:\n",
    "                df = pd.DataFrame([data])\n",
    "            \n",
    "            # ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ ë° ë³€í™˜\n",
    "            if 'article' in df.columns and 'text' not in df.columns:\n",
    "                df['text'] = df['article']\n",
    "            if 'summary' not in df.columns and 'abstractive' in df.columns:\n",
    "                df['summary'] = df['abstractive']\n",
    "        \n",
    "        # ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ìš”ì•½ì´ ìˆëŠ” í–‰ ì œê±°\n",
    "        df = df.dropna(subset=['text'])\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        \n",
    "        # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ\n",
    "        if len(df) > max_samples:\n",
    "            df = df.sample(max_samples, random_state=42)\n",
    "        \n",
    "        print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(df)} ìƒ˜í”Œ\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±\n",
    "        print(\"ì˜ˆì‹œ ë°ì´í„° ì‚¬ìš©\")\n",
    "        example_data = {\n",
    "            'text': [\n",
    "                \"í•œêµ­ì€í–‰ì´ í†µí™”ì •ì±… ê²°ì • íšŒì˜ì—ì„œ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ ë™ê²°í–ˆë‹¤. í•œêµ­ì€í–‰ì€ ì§€ë‚œí•´ 4ë¶„ê¸° ì´í›„ ê³„ì†ëœ ê²½ê¸°ì¹¨ì²´ì˜ ì˜í–¥ìœ¼ë¡œ ê³ ìš© ì‹œì¥ì´ ìœ„ì¶•ë˜ê³  ìˆìœ¼ë©°, ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ëª©í‘œ ìˆ˜ì¤€ìœ¼ë¡œ ì•ˆì •ë˜ê³  ìˆë‹¤ê³  íŒë‹¨í–ˆë‹¤. ì‹œì¥ ì „ë¬¸ê°€ë“¤ì€ í•œêµ­ ê²½ì œì˜ íšŒë³µì„¸ê°€ ì˜ˆìƒë³´ë‹¤ ë”ë””ê²Œ ì§„í–‰ë˜ê³  ìˆì–´ ê¸ˆë¦¬ ì¸í•˜ ê°€ëŠ¥ì„±ë„ ìˆë‹¤ê³  ì „ë§í–ˆë‹¤.\",\n",
    "                \"ê¸ˆìœµìœ„ì›íšŒëŠ” ì˜¤ëŠ˜ ê°€ê³„ë¶€ì±„ ê´€ë¦¬ ë°©ì•ˆì„ ë°œí‘œí–ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ì´ë¶€ì±„ì›ë¦¬ê¸ˆìƒí™˜ë¹„ìœ¨(DSR) ê·œì œë¥¼ 40%ë¡œ ê°•í™”í•˜ê³ , ë‹¤ì£¼íƒìì— ëŒ€í•œ ì£¼íƒë‹´ë³´ëŒ€ì¶œ ì œí•œì„ í™•ëŒ€í•˜ëŠ” ê²ƒì´ë‹¤. ë˜í•œ ì‹¤ìˆ˜ìš”ìì— ëŒ€í•œ ëŒ€ì¶œ ì§€ì›ì€ í™•ëŒ€í•˜ë˜, íˆ¬ê¸° ëª©ì ì˜ ëŒ€ì¶œì—ëŠ” ì œì¬ë¥¼ ê°•í™”í•˜ëŠ” íˆ¬íŠ¸ë™ ì „ëµì„ í¼ì¹˜ê¸°ë¡œ í–ˆë‹¤.\"\n",
    "            ],\n",
    "            'summary': [\n",
    "                \"í•œêµ­ì€í–‰ì´ í†µí™”ì •ì±… ê²°ì • íšŒì˜ì—ì„œ ê¸°ì¤€ê¸ˆë¦¬ ë™ê²°, ê²½ê¸°ì¹¨ì²´ë¡œ ì¸í•œ ê³ ìš©ì‹œì¥ ìœ„ì¶•ê³¼ ë¬¼ê°€ ì•ˆì • íŒë‹¨\",\n",
    "                \"ê¸ˆìœµìœ„, ê°€ê³„ë¶€ì±„ ê´€ë¦¬ë°©ì•ˆ ë°œí‘œ - DSR 40% ê°•í™”, ë‹¤ì£¼íƒì ëŒ€ì¶œì œí•œ í™•ëŒ€, ì‹¤ìˆ˜ìš”ì ì§€ì› í™•ëŒ€\"\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAPìš© íŒŒì´í”„ë¼ì¸ ë° Explainer êµ¬ì„±\n",
    "class SummarizationPipeline:\n",
    "    def __init__(self, model, tokenizer, embedder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.original_text = None\n",
    "        self.reference_summary = None\n",
    "        self.perturbation_count = 0\n",
    "    \n",
    "    def set_reference(self, text, summary=None):\n",
    "        \"\"\"ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° ìš”ì•½ ì„¤ì •\"\"\"\n",
    "        self.original_text = text\n",
    "        self.perturbation_count = 0\n",
    "        \n",
    "        if summary is None:\n",
    "            # ì°¸ì¡° ìš”ì•½ì´ ì—†ìœ¼ë©´ ëª¨ë¸ë¡œ ìƒì„±\n",
    "            print(\"ì°¸ì¡° ìš”ì•½ ìƒì„± ì¤‘...\")\n",
    "            self.reference_summary = summarize_text(text)\n",
    "            print(f\"ìƒì„±ëœ ì°¸ì¡° ìš”ì•½: {self.reference_summary}\")\n",
    "        else:\n",
    "            self.reference_summary = summary\n",
    "            print(f\"ì œê³µëœ ì°¸ì¡° ìš”ì•½: {self.reference_summary}\")\n",
    "        \n",
    "        # ì°¸ì¡° ìš”ì•½ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°\n",
    "        self.reference_embedding = self.embedder.encode(\n",
    "            self.reference_summary, convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        SHAPìš© í˜¸ì¶œ í•¨ìˆ˜. Perturbationëœ í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë°›ì•„ ê°ê°ì˜ ìš”ì•½ í’ˆì§ˆ ì ìˆ˜ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        batch_size = min(2, len(texts))  # ë°°ì¹˜ í¬ê¸° ì¤„ì„(ë©”ëª¨ë¦¬ ê³ ë ¤)\n",
    "        all_scores = []\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        # tqdm ì§„í–‰ ë°” ì„¤ì •\n",
    "        total = getattr(self, \"_expected_perturbations\", 100)\n",
    "        pbar = tqdm(total=total, desc=\"ğŸ” SHAP perturbation ì§„í–‰\", unit=\"step\", leave=True)\n",
    "        pbar.n = self.perturbation_count  # ì´ì „ì— ì§„í–‰ëœ ìˆ˜ ë°˜ì˜\n",
    "        pbar.refresh()\n",
    "         \n",
    "                \n",
    "        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_scores = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                # ë„˜íŒŒì´ ë°°ì—´ì´ë‚˜ í…ì„œ ì²˜ë¦¬\n",
    "                if isinstance(text, (np.ndarray, torch.Tensor)):\n",
    "                    text = text.tolist()\n",
    "\n",
    "                # ë¦¬ìŠ¤íŠ¸ â†’ ì²« ì›ì†Œ êº¼ëƒ„\n",
    "                if isinstance(text, list) and len(text) > 0:\n",
    "                    text = text[0]\n",
    "\n",
    "                # ë¬¸ìì—´ì¸ì§€ ìµœì¢… í™•ì¸\n",
    "                if not isinstance(text, str):\n",
    "                    print(f\"[!] í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŒ: {type(text)}\")\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                \n",
    "                # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬\n",
    "                if not text.strip():\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # ìš”ì•½ ìƒì„±\n",
    "                    summary = summarize_text(text)\n",
    "                    \n",
    "                    if not summary.strip():\n",
    "                        print(\"[!] ìš”ì•½ ê²°ê³¼ê°€ ê³µë°±ì…ë‹ˆë‹¤. ì ìˆ˜ ê³„ì‚° ìƒëµ\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # ìƒì„±ëœ ìš”ì•½ì˜ ì„ë² ë”© ê³„ì‚°\n",
    "                    summary_embedding = self.embedder.encode(\n",
    "                        summary, convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "                    # NaN ì²´í¬ ë° ì²˜ë¦¬\n",
    "                    if torch.isnan(summary_embedding).any() or torch.isnan(self.reference_embedding).any():\n",
    "                        print(f\"[!] NaN detected in embeddings. ìš”ì•½ í…ìŠ¤íŠ¸: {summary}\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # ì°¸ì¡° ìš”ì•½ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
    "                    similarity = util.cos_sim(\n",
    "                        summary_embedding, self.reference_embedding\n",
    "                    ).item()\n",
    "                    \n",
    "                    # ì ìˆ˜ ë²”ìœ„ ë³´ì •\n",
    "                    normalized_score = max(0.0, min(1.0, float(similarity)))\n",
    "                    batch_scores.append(normalized_score)\n",
    "\n",
    "                    self.perturbation_count += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[!] ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}\")\n",
    "                    batch_scores.append(0.0)  # ì˜¤ë¥˜ ì‹œ 0ì  ì²˜ë¦¬\n",
    "            \n",
    "            all_scores.extend(batch_scores)\n",
    "                    \n",
    "            # ğŸ” ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€\n",
    "            print(f\"[DEBUG] texts ê¸¸ì´: {len(texts)}\")\n",
    "            print(f\"[DEBUG] í˜„ì¬ batch_texts ê¸¸ì´: {len(batch_texts)}\")\n",
    "            print(f\"[DEBUG] batch_scores ê¸¸ì´: {len(batch_scores)}\")\n",
    "            print(f\"[DEBUG] ëˆ„ì  all_scores ê¸¸ì´: {len(all_scores)}\")\n",
    "    \n",
    "        # ë¬´ì¡°ê±´ ì…ë ¥ ê°œìˆ˜ì™€ ì¶œë ¥ ê°œìˆ˜ ë§ì¶°ì£¼ê¸°\n",
    "        if len(all_scores) != len(texts):\n",
    "            print(f\"[!] Warning: ì…ë ¥ ìˆ˜({len(texts)})ì™€ ì¶œë ¥ ìˆ˜({len(all_scores)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤. 0.0ìœ¼ë¡œ ì±„ì›€\")\n",
    "            if len(all_scores) > len(texts):\n",
    "                # ë„ˆë¬´ ë§ì´ ë§Œë“  ê²½ìš° ìë¥´ê¸°\n",
    "                all_scores = all_scores[:len(texts)]\n",
    "            else:\n",
    "                # ë¶€ì¡±í•œ ê²½ìš° 0.0ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "                while len(all_scores) < len(texts):\n",
    "                    all_scores.append(0.0)\n",
    "\n",
    "        if len(texts) == 1:\n",
    "            return [float(all_scores[0])]  # SHAPì´ 1ê°œë§Œ ìš”ì²­í•œ ê²½ìš°\n",
    "        else:\n",
    "            return [float(s) for s in all_scores]\n",
    "    \n",
    "    def explain_specific_tokens(self, tokens, top_n=5):\n",
    "        \"\"\"\n",
    "        íŠ¹ì • í† í°ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        original_summary = summarize_text(self.original_text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            # í•´ë‹¹ í† í°ì„ ì œê±°í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            removed_text = self.original_text.replace(token, \"\")\n",
    "            \n",
    "            # ì œê±° í›„ ìš”ì•½ ìƒì„±\n",
    "            removed_summary = summarize_text(removed_text)\n",
    "            \n",
    "            # ì›ë³¸ ìš”ì•½ê³¼ ì œê±° í›„ ìš”ì•½ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "            original_emb = self.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "            removed_emb = self.embedder.encode(removed_summary, convert_to_tensor=True)\n",
    "            \n",
    "            # ìœ ì‚¬ë„ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•´ë‹¹ í† í°ì´ ì¤‘ìš”í•¨\n",
    "            similarity = util.cos_sim(original_emb, removed_emb).item()\n",
    "            importance = 1.0 - similarity\n",
    "            \n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"importance\": importance,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"removed_summary\": removed_summary\n",
    "            })\n",
    "            \n",
    "        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        results.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "\n",
    "def analyze_with_shap(text, reference_summary=None, num_samples=100, verbose=True):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ SHAP ë¶„ì„ ìˆ˜í–‰\n",
    "    \n",
    "    Args:\n",
    "        text: ë¶„ì„í•  ì›ë¬¸ í…ìŠ¤íŠ¸\n",
    "        reference_summary: ì°¸ì¡° ìš”ì•½ (ì—†ìœ¼ë©´ ëª¨ë¸ë¡œ ìƒì„±)\n",
    "        num_samples: SHAP ìƒ˜í”Œë§ ìˆ˜\n",
    "        verbose: ìì„¸í•œ ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        shap_values: SHAP ê°’\n",
    "        summary: ìƒì„±ëœ ìš”ì•½\n",
    "        pipeline: ë¶„ì„ì— ì‚¬ìš©ëœ íŒŒì´í”„ë¼ì¸ ê°ì²´\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\nì›ë¬¸ ë¶„ì„ ì‹œì‘\\n{'='*80}\")\n",
    "        print(f\"ì›ë¬¸ (ì¼ë¶€): {text[:200]}...\")\n",
    "    \n",
    "    # ë¹ˆ í…ìŠ¤íŠ¸ í™•ì¸\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        raise ValueError(\"ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì°¸ì¡° ì„¤ì •\n",
    "    pipeline = SummarizationPipeline(model, tokenizer, embedder)\n",
    "    pipeline.set_reference(text, reference_summary)\n",
    "    \n",
    "    # SHAP Explainer ìƒì„±\n",
    "    try:\n",
    "        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "        words = text.split()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP Explainer ì´ˆê¸°í™” ì¤‘...\")\n",
    "            print(f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}, ë‹¨ì–´ ìˆ˜: {len(words)}\")\n",
    "        \n",
    "        mask_token = tokenizer.pad_token or tokenizer.eos_token or \"â€¦\"  # fallback\n",
    "        masker = shap.maskers.Text(tokenizer=tokenizer, mask_token=mask_token)          \n",
    "        \n",
    "        # Partition ë§ˆìŠ¤ì»¤ ì‚¬ìš© (ë” ì•ˆì •ì )\n",
    "        explainer = shap.Explainer(pipeline, masker)\n",
    "        \n",
    "        # ìƒ˜í”Œ ìˆ˜ ìë™ ì¡°ì • (ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "        num_features = len(words)\n",
    "        adjusted_samples = min(max(2 * num_features + 1, 50), num_samples)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP ê°’ ê³„ì‚° ì¤‘ (ë‹¨ì–´ ìˆ˜: {num_features}, ìƒ˜í”Œ ìˆ˜: {adjusted_samples})...\")\n",
    "        \n",
    "        # SHAP ê°’ ê³„ì‚° - ë‹¨ì¼ ë°ì´í„°ë¡œ ëª…ì‹œ\n",
    "        shap_values = explainer([text], max_evals=30)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"SHAP ë¶„ì„ ì™„ë£Œ\")\n",
    "            print(f\"SHAP ê°’ í˜•íƒœ: {shap_values.values.shape}\")\n",
    "        \n",
    "        return shap_values, pipeline.reference_summary, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"ë” ë‹¨ìˆœí•œ ì„¤ì •ìœ¼ë¡œ SHAP ë¶„ì„ ì¬ì‹œë„...\")\n",
    "        try:\n",
    "            # ë” ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ì ‘ê·¼ë°©ì‹ ì‚¬ìš©\n",
    "            # í…ìŠ¤íŠ¸ ì§ì ‘ ì²˜ë¦¬\n",
    "            tokenized_text = text.split()\n",
    "            \n",
    "            # ìˆ˜ë™ìœ¼ë¡œ SHAP ê°’ ìƒì„±\n",
    "            dummy_values = np.zeros((1, len(tokenized_text)))\n",
    "            \n",
    "            # ê° í† í°ì˜ ì¤‘ìš”ë„ë¥¼ ê°„ë‹¨íˆ ê³„ì‚° (ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜)\n",
    "            original_summary = summarize_text(text)\n",
    "            \n",
    "            for i, token in enumerate(tokenized_text):\n",
    "                # í•´ë‹¹ í† í° ì œê±°\n",
    "                modified_text = ' '.join([t for j, t in enumerate(tokenized_text) if j != i])\n",
    "                \n",
    "                try:\n",
    "                    # ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "                    modified_summary = summarize_text(modified_text)\n",
    "                    \n",
    "                    # ë‘ ìš”ì•½ì˜ ì„ë² ë”© ë¹„êµ\n",
    "                    orig_emb = pipeline.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "                    mod_emb = pipeline.embedder.encode(modified_summary, convert_to_tensor=True)\n",
    "                    \n",
    "                    # ìœ ì‚¬ë„ ê³„ì‚° (1 - ìœ ì‚¬ë„ = ì¤‘ìš”ë„)\n",
    "                    similarity = util.cos_sim(orig_emb, mod_emb).item()\n",
    "                    importance = 1.0 - similarity\n",
    "                    \n",
    "                    # ì¤‘ìš”ë„ ì €ì¥\n",
    "                    dummy_values[0, i] = importance\n",
    "                except Exception as e2:\n",
    "                    print(f\"í† í° '{token}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e2}\")\n",
    "                    dummy_values[0, i] = 0.0\n",
    "            \n",
    "            # SHAP ê²°ê³¼ì™€ ìœ ì‚¬í•œ ê°ì²´ ìƒì„±\n",
    "            class CustomShapValues:\n",
    "                def __init__(self, values, data, feature_names):\n",
    "                    self.values = values\n",
    "                    self.data = data\n",
    "                    self.feature_names = feature_names\n",
    "                    self.output_names = [\"importance\"]\n",
    "                    self.base_values = np.zeros(1)\n",
    "            \n",
    "            # ê²°ê³¼ ë°˜í™˜\n",
    "            shap_obj = CustomShapValues(\n",
    "                values=dummy_values,\n",
    "                data=np.array([text]),\n",
    "                feature_names=tokenized_text\n",
    "            )\n",
    "            \n",
    "            print(\"ëŒ€ì²´ SHAP ë¶„ì„ ì™„ë£Œ\")\n",
    "            return shap_obj, original_summary, pipeline\n",
    "            \n",
    "        except Exception as retry_error:\n",
    "            print(f\"ëŒ€ì²´ SHAP ë¶„ì„ë„ ì‹¤íŒ¨: {retry_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise ValueError(\"ëª¨ë“  SHAP ë¶„ì„ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def print_important_features(shap_values, summary, top_n=10):\n",
    "    \"\"\"\n",
    "    SHAP ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” featureë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    # SHAP ê°’ì˜ ì ˆëŒ€ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    token_importances = []\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬\n",
    "        if hasattr(shap_values, 'data') and isinstance(shap_values.data, np.ndarray):\n",
    "            text_data = shap_values.data[0]\n",
    "            if isinstance(text_data, np.ndarray) and text_data.size == 1:\n",
    "                text_data = text_data.item()\n",
    "        else:\n",
    "            # ëŒ€ì²´ ë°©ì‹\n",
    "            text_data = shap_values.data[0] if hasattr(shap_values, 'data') else \"í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ\"\n",
    "        \n",
    "        # í† í° ë¶„í• \n",
    "        tokens = text_data.split() if isinstance(text_data, str) else []\n",
    "        \n",
    "        # í† í°ê³¼ SHAP ê°’ ë§¤í•‘\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i < shap_values.values.shape[1]:  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸\n",
    "                importance = abs(shap_values.values[0][i])\n",
    "                raw_value = shap_values.values[0][i]\n",
    "                token_importances.append((token, importance, raw_value))\n",
    "        \n",
    "        # ì¤‘ìš”ë„ ê¸°ì¤€ ì •ë ¬\n",
    "        sorted_importances = sorted(token_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ìƒì„±ëœ ìš”ì•½: {summary}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ìƒìœ„ {top_n}ê°œ ì¤‘ìš” ë‹¨ì–´ (SHAP ê¸°ì¤€):\")\n",
    "        print(f\"{'ë‹¨ì–´':<15} | {'ì¤‘ìš”ë„':>10} | {'ì˜í–¥':>10} | {'í•´ì„'}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        for token, importance, raw_value in sorted_importances[:top_n]:\n",
    "            effect = \"ê¸ì •ì  ì˜í–¥\" if raw_value > 0 else \"ë¶€ì •ì  ì˜í–¥\"\n",
    "            print(f\"{token:<15} | {importance:>10.4f} | {raw_value:>10.4f} | {effect}\")\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ì¤‘ìš” íŠ¹ì„± ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"ê°„ì†Œí™”ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥:\")\n",
    "        print(f\"ìš”ì•½: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: ./data/train_original.json\n",
      "ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: 10 ìƒ˜í”Œ\n",
      "\n",
      "ì´ 10ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘\n",
      "\n",
      "ìƒ˜í”Œ 1/10 ë¶„ì„:\n",
      "\n",
      "================================================================================\n",
      "ì›ë¬¸ ë¶„ì„ ì‹œì‘\n",
      "================================================================================\n",
      "ì›ë¬¸ (ì¼ë¶€): ìµœëª…êµ­ ì†¡í•˜ì§„ ì§€ì‚¬, ë°©ì¤‘ ì£¼ìš” ì„±ê³¼ë¡œ 'êµ°ì‚°~ì—°ìš´í•­' í•­ë¡œ ê°œì„¤ í˜‘ì˜ ê¼½ì•„ ì¥ì‘¤ì„± ë‹¹ì„œê¸° \"ë°”ë‹·ê¸¸ í†µí•œ í˜‘ë ¥, ì ê·¹ ê²€í† \" ë‘ ì§€ì—­ ì¸ì  êµë¥˜ í™œë°œ, ì§€ë‚œí•´ ì„ë„ í•­ë¡œ ì¦í¸ ì†¡í•˜ì§„ ì „ë¶ë„ì§€ì‚¬ê°€ ì¤‘êµ­ ì¥ì‘¤ì„±(ê°•ì†Œì„±) ë°©ë¬¸ì˜ ì£¼ìš” ì„±ê³¼ë¡œ ì¤‘êµ­ ì¸¡ê³¼ì˜ 'êµ°ì‚°~ì¥ì‘¤ì„± ì—°ìš´í•­' í•­ë¡œ ê°œì„¤ í˜‘ì˜ë¥¼ ê¼½ì•˜ë‹¤. ì†¡í•˜ì§„ ë„ì§€ì‚¬ëŠ” 1ì¼ ì¶œì…ê¸°ìë“¤ê³¼ ë§Œë‚˜ \"êµ°ì‚°ê³¼ ì¥ì‘¤ì„± ì—°ìš´...\n",
      "ì œê³µëœ ì°¸ì¡° ìš”ì•½: ì†¡í•˜ì§„ ì „ë¶ë„ì§€ì‚¬ê°€ ì¤‘êµ­ ì¥ì‘¤ì„±ì„ ë°©ë¬¸í•´ ëŸ¬ìš° ì¹œì§€ì•¤ ë‹¹ì„œê¸°ì™€ ìƒˆë§Œê¸ˆ ì‚°ë‹¨ 5ê³µêµ¬ ê³µë™íˆ¬ì í™œìš©ì•ˆì— ëŒ€í•´ í˜‘ì˜í•˜ê³  êµ°ì‚°ê³¼ ì¥ì‘¤ì„± ì—°ìš´í•­ ê°„ ì‹ ê·œ ì—¬ê° í•­ë¡œ ê°œì„¤ì„ í†µí•œ í˜‘ë ¥ë°©ì•ˆì„ ë…¼ì˜í–ˆë‹¤.\n",
      "SHAP Explainer ì´ˆê¸°í™” ì¤‘...\n",
      "í…ìŠ¤íŠ¸ ê¸¸ì´: 892, ë‹¨ì–´ ìˆ˜: 219\n",
      "SHAP ê°’ ê³„ì‚° ì¤‘ (ë‹¨ì–´ ìˆ˜: 219, ìƒ˜í”Œ ìˆ˜: 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   0%|          | 0/100 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 38])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   1%|          | 1/100 [11:37<19:11:17, 697.75s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Warning: ì…ë ¥ ìˆ˜(1)ì™€ ì¶œë ¥ ìˆ˜(2)ê°€ ë‹¤ë¦…ë‹ˆë‹¤. 0.0ìœ¼ë¡œ ì±„ì›€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   1%|          | 1/100 [00:00<00:00, 155344.59step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 595])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 595])\n"
     ]
    }
   ],
   "source": [
    "# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"./data/train_original.json\" # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½\n",
    "    df = load_dataset(dataset_path, max_samples=10)  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° ìš”ì•½ ì¶”ì¶œ\n",
    "    texts = df[\"text\"].tolist()\n",
    "    references = df[\"summary\"].tolist() if \"summary\" in df.columns else [None] * len(texts)\n",
    "    \n",
    "    print(f\"\\nì´ {len(texts)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘\")\n",
    "    \n",
    "    # ê° ìƒ˜í”Œì— ëŒ€í•´ SHAP ë¶„ì„ ìˆ˜í–‰\n",
    "    for i, (text, reference) in enumerate(zip(texts, references)):\n",
    "        print(f\"\\nìƒ˜í”Œ {i+1}/{len(texts)} ë¶„ì„:\")\n",
    "        \n",
    "        try:\n",
    "            shap_values, summary, _ = analyze_with_shap(\n",
    "                text, \n",
    "                reference_summary=reference,\n",
    "                num_samples=5,  # ìƒ˜í”Œë§ ìˆ˜ ì¡°ì • (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # ì¤‘ìš” feature ì¶œë ¥\n",
    "            print_important_features(shap_values, summary, top_n=15)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ìƒ˜í”Œ {i+1} ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"ì´ ìƒ˜í”Œì€ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nëª¨ë“  ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
