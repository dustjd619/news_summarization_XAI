{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8028de4c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#debugging용 설정\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 토크나이저 로딩 중...\n",
      "CUDA 메모리 정리 완료. 사용 가능한 GPU: 1개\n",
      "현재 GPU 메모리 사용량: 0.00 MB\n",
      "27B 모델 로드 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15edad3f2624e8f8f8356fc3900828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료 (device: cuda:0)\n",
      "임베딩 모델 로딩 중...\n",
      "임베딩 모델 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 및 토크나이저 로드\n",
    "print(\"모델 및 토크나이저 로딩 중...\")\n",
    "model_name = \"google/gemma-3-27b-it\"  # Gemma 3 27B 모델 (instruction tuned)\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# CUDA 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"CUDA 메모리 정리 완료. 사용 가능한 GPU: {torch.cuda.device_count()}개\")\n",
    "    print(f\"현재 GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 모델 로드 시도\n",
    "try:\n",
    "    print(f\"27B 모델 로드 중...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # 자동으로 적절한 장치에 할당\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"모델 로드 완료 (device: {device})\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU 로드 실패: {e}\")\n",
    "    print(\"CPU로 대체 모델 로드 중...\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # CPU로 다시 시도\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"CPU 모드로 모델 로드 완료 (device: {device})\")\n",
    "\n",
    "# sentence embedding 모델 로드 (다국어 지원 모델)\n",
    "print(\"임베딩 모델 로딩 중...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"임베딩 모델 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 프롬프트 기반 text generation 함수\n",
    "def summarize_text(text):\n",
    "    \"\"\"텍스트를 요약하는 함수\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gemma 모델용 instruction 형식 프롬프트\n",
    "    prompt = f\"<start_of_turn>user\\n경제금융 뉴스 기사를 요약해주세요. 중요한 단어 및 내용인 무엇인지 고려해서 요약해주세요.\\n\\n기사: {text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # 토크나이징 및 입력 준비\n",
    "    max_input_length = 2048-128\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"[DEBUG] input_ids shape:\", inputs[\"input_ids\"].shape)\n",
    "    print(\"[DEBUG] input_ids (앞 30개):\", inputs[\"input_ids\"][0][:30]) \n",
    "    \n",
    "    # 생성 설정\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "    \n",
    "    # 요약 생성\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\">>> input shape:\", inputs[\"input_ids\"].shape)\n",
    "\n",
    "            output = model.generate(**inputs, **gen_config)  # GPU에서 생성\n",
    "            if output.dim() > 1:\n",
    "                output = output[0]  # 첫 번째 결과만 사용 (batch_size=1)\n",
    "\n",
    "            if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "                print(\"[!] generate() 결과에 nan 또는 inf 포함됨 — 디코딩 중단\")\n",
    "                return \"\"\n",
    "\n",
    "            output = output.to(\"cpu\")  # CPU로 옮겨서 안전하게 디코딩\n",
    "            full_response = tokenizer.decode(output, skip_special_tokens=False)  # ✅ 여기 수정!\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] generate() 오류: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    # 응답 파싱 (모델 응답 부분만 추출)\n",
    "    try:\n",
    "        # Gemma 응답 형식에서 모델 응답 부분만 추출\n",
    "        if \"<start_of_turn>model\" in full_response:\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\\n\")[1]\n",
    "            if \"<end_of_turn>\" in response_parts:\n",
    "                summary = response_parts.split(\"<end_of_turn>\")[0].strip()\n",
    "            else:\n",
    "                summary = response_parts.strip()\n",
    "        else:\n",
    "            # 프롬프트 제거\n",
    "            summary = full_response.replace(prompt, \"\").strip()\n",
    "            \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"요약 파싱 오류: {e}\")\n",
    "        # 오류 발생 시 전체 응답에서 프롬프트 부분 제거 시도\n",
    "        return full_response.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터셋 가져오기\n",
    "def load_dataset(file_path, max_samples=50):\n",
    "    \"\"\"JSON 파일에서 뉴스 문서와 요약 데이터셋 로드\"\"\"\n",
    "    print(f\"데이터셋 로딩 중: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 데이터셋 구조 확인\n",
    "        if \"documents\" in data:\n",
    "            # 문서 데이터 추출\n",
    "            documents = data[\"documents\"]\n",
    "            processed_data = []\n",
    "            \n",
    "            for doc in documents:\n",
    "                # 원문 텍스트 추출 (text는 배열 구조)\n",
    "                original_text = \"\"\n",
    "                if \"text\" in doc:\n",
    "                    # 문장들을 하나의 텍스트로 합치기\n",
    "                    for paragraph in doc[\"text\"]:\n",
    "                        for sent_obj in paragraph:\n",
    "                            if \"sentence\" in sent_obj:\n",
    "                                original_text += sent_obj[\"sentence\"] + \" \"\n",
    "                \n",
    "                # 요약문 추출\n",
    "                summary = \"\"\n",
    "                if \"abstractive\" in doc and doc[\"abstractive\"]:\n",
    "                    summary = doc[\"abstractive\"][0] if isinstance(doc[\"abstractive\"], list) else doc[\"abstractive\"]\n",
    "                \n",
    "                # 필요한 정보만 추출하여 데이터프레임용 딕셔너리 생성\n",
    "                processed_doc = {\n",
    "                    \"id\": doc.get(\"id\", \"\"),\n",
    "                    \"title\": doc.get(\"title\", \"\"),\n",
    "                    \"text\": original_text.strip(),\n",
    "                    \"summary\": summary,\n",
    "                    \"category\": doc.get(\"category\", \"\"),\n",
    "                    \"media_name\": doc.get(\"media_name\", \"\")\n",
    "                }\n",
    "                \n",
    "                processed_data.append(processed_doc)\n",
    "            \n",
    "            # 데이터프레임 생성\n",
    "            df = pd.DataFrame(processed_data)\n",
    "            \n",
    "        else:\n",
    "            # 다른 구조의 JSON 처리\n",
    "            print(\"표준 구조가 아닌 JSON 파일입니다.\")\n",
    "            if isinstance(data, list):\n",
    "                df = pd.DataFrame(data)\n",
    "            else:\n",
    "                df = pd.DataFrame([data])\n",
    "            \n",
    "            # 컬럼 이름 확인 및 변환\n",
    "            if 'article' in df.columns and 'text' not in df.columns:\n",
    "                df['text'] = df['article']\n",
    "            if 'summary' not in df.columns and 'abstractive' in df.columns:\n",
    "                df['summary'] = df['abstractive']\n",
    "        \n",
    "        # 빈 텍스트나 요약이 있는 행 제거\n",
    "        df = df.dropna(subset=['text'])\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        \n",
    "        # 최대 샘플 수 제한\n",
    "        if len(df) > max_samples:\n",
    "            df = df.sample(max_samples, random_state=42)\n",
    "        \n",
    "        print(f\"데이터셋 로딩 완료: {len(df)} 샘플\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"데이터셋 로딩 실패: {e}\")\n",
    "        # 예시 데이터 생성\n",
    "        print(\"예시 데이터 사용\")\n",
    "        example_data = {\n",
    "            'text': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리를 동결했다. 한국은행은 지난해 4분기 이후 계속된 경기침체의 영향으로 고용 시장이 위축되고 있으며, 물가상승률은 목표 수준으로 안정되고 있다고 판단했다. 시장 전문가들은 한국 경제의 회복세가 예상보다 더디게 진행되고 있어 금리 인하 가능성도 있다고 전망했다.\",\n",
    "                \"금융위원회는 오늘 가계부채 관리 방안을 발표했다. 주요 내용은 총부채원리금상환비율(DSR) 규제를 40%로 강화하고, 다주택자에 대한 주택담보대출 제한을 확대하는 것이다. 또한 실수요자에 대한 대출 지원은 확대하되, 투기 목적의 대출에는 제재를 강화하는 투트랙 전략을 펼치기로 했다.\"\n",
    "            ],\n",
    "            'summary': [\n",
    "                \"한국은행이 통화정책 결정 회의에서 기준금리 동결, 경기침체로 인한 고용시장 위축과 물가 안정 판단\",\n",
    "                \"금융위, 가계부채 관리방안 발표 - DSR 40% 강화, 다주택자 대출제한 확대, 실수요자 지원 확대\"\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAP용 파이프라인 및 Explainer 구성\n",
    "class SummarizationPipeline:\n",
    "    def __init__(self, model, tokenizer, embedder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.original_text = None\n",
    "        self.reference_summary = None\n",
    "        self.perturbation_count = 0\n",
    "    \n",
    "    def set_reference(self, text, summary=None):\n",
    "        \"\"\"원본 텍스트와 참조 요약 설정\"\"\"\n",
    "        self.original_text = text\n",
    "        self.perturbation_count = 0\n",
    "        \n",
    "        if summary is None:\n",
    "            # 참조 요약이 없으면 모델로 생성\n",
    "            print(\"참조 요약 생성 중...\")\n",
    "            self.reference_summary = summarize_text(text)\n",
    "            print(f\"생성된 참조 요약: {self.reference_summary}\")\n",
    "        else:\n",
    "            self.reference_summary = summary\n",
    "            print(f\"제공된 참조 요약: {self.reference_summary}\")\n",
    "        \n",
    "        # 참조 요약 임베딩 미리 계산\n",
    "        self.reference_embedding = self.embedder.encode(\n",
    "            self.reference_summary, convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        SHAP용 호출 함수. Perturbation된 텍스트 목록을 받아 각각의 요약 품질 점수 반환\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        batch_size = min(2, len(texts))  # 배치 크기 줄임(메모리 고려)\n",
    "        all_scores = []\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        # tqdm 진행 바 설정\n",
    "        total = getattr(self, \"_expected_perturbations\", 100)\n",
    "        pbar = tqdm(total=total, desc=\"🔍 SHAP perturbation 진행\", unit=\"step\", leave=True)\n",
    "        pbar.n = self.perturbation_count  # 이전에 진행된 수 반영\n",
    "        pbar.refresh()\n",
    "         \n",
    "                \n",
    "        # 배치 단위로 처리\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_scores = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                # 넘파이 배열이나 텐서 처리\n",
    "                if isinstance(text, (np.ndarray, torch.Tensor)):\n",
    "                    text = text.tolist()\n",
    "\n",
    "                # 리스트 → 첫 원소 꺼냄\n",
    "                if isinstance(text, list) and len(text) > 0:\n",
    "                    text = text[0]\n",
    "\n",
    "                # 문자열인지 최종 확인\n",
    "                if not isinstance(text, str):\n",
    "                    print(f\"[!] 텍스트를 문자열로 변환할 수 없음: {type(text)}\")\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                \n",
    "                # 빈 텍스트 체크\n",
    "                if not text.strip():\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # 요약 생성\n",
    "                    summary = summarize_text(text)\n",
    "                    \n",
    "                    if not summary.strip():\n",
    "                        print(\"[!] 요약 결과가 공백입니다. 점수 계산 생략\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # 생성된 요약의 임베딩 계산\n",
    "                    summary_embedding = self.embedder.encode(\n",
    "                        summary, convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "                    # NaN 체크 및 처리\n",
    "                    if torch.isnan(summary_embedding).any() or torch.isnan(self.reference_embedding).any():\n",
    "                        print(f\"[!] NaN detected in embeddings. 요약 텍스트: {summary}\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # 참조 요약과의 유사도 계산 (코사인 유사도)\n",
    "                    similarity = util.cos_sim(\n",
    "                        summary_embedding, self.reference_embedding\n",
    "                    ).item()\n",
    "                    \n",
    "                    # 점수 범위 보정\n",
    "                    normalized_score = max(0.0, min(1.0, float(similarity)))\n",
    "                    batch_scores.append(normalized_score)\n",
    "\n",
    "                    self.perturbation_count += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[!] 점수 계산 오류: {str(e)}\")\n",
    "                    batch_scores.append(0.0)  # 오류 시 0점 처리\n",
    "            \n",
    "            all_scores.extend(batch_scores)\n",
    "                    \n",
    "            # 🔍 디버깅 로그 추가\n",
    "            print(f\"[DEBUG] texts 길이: {len(texts)}\")\n",
    "            print(f\"[DEBUG] 현재 batch_texts 길이: {len(batch_texts)}\")\n",
    "            print(f\"[DEBUG] batch_scores 길이: {len(batch_scores)}\")\n",
    "            print(f\"[DEBUG] 누적 all_scores 길이: {len(all_scores)}\")\n",
    "    \n",
    "        # 무조건 입력 개수와 출력 개수 맞춰주기\n",
    "        if len(all_scores) != len(texts):\n",
    "            print(f\"[!] Warning: 입력 수({len(texts)})와 출력 수({len(all_scores)})가 다릅니다. 0.0으로 채움\")\n",
    "            if len(all_scores) > len(texts):\n",
    "                # 너무 많이 만든 경우 자르기\n",
    "                all_scores = all_scores[:len(texts)]\n",
    "            else:\n",
    "                # 부족한 경우 0.0으로 채우기\n",
    "                while len(all_scores) < len(texts):\n",
    "                    all_scores.append(0.0)\n",
    "\n",
    "        if len(texts) == 1:\n",
    "            return [float(all_scores[0])]  # SHAP이 1개만 요청한 경우\n",
    "        else:\n",
    "            return [float(s) for s in all_scores]\n",
    "    \n",
    "    def explain_specific_tokens(self, tokens, top_n=5):\n",
    "        \"\"\"\n",
    "        특정 토큰들의 중요도를 개별적으로 분석\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        original_summary = summarize_text(self.original_text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            # 해당 토큰을 제거한 텍스트 생성\n",
    "            removed_text = self.original_text.replace(token, \"\")\n",
    "            \n",
    "            # 제거 후 요약 생성\n",
    "            removed_summary = summarize_text(removed_text)\n",
    "            \n",
    "            # 원본 요약과 제거 후 요약의 유사도 계산\n",
    "            original_emb = self.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "            removed_emb = self.embedder.encode(removed_summary, convert_to_tensor=True)\n",
    "            \n",
    "            # 유사도 차이가 클수록 해당 토큰이 중요함\n",
    "            similarity = util.cos_sim(original_emb, removed_emb).item()\n",
    "            importance = 1.0 - similarity\n",
    "            \n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"importance\": importance,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"removed_summary\": removed_summary\n",
    "            })\n",
    "            \n",
    "        # 중요도 순으로 정렬\n",
    "        results.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "\n",
    "def analyze_with_shap(text, reference_summary=None, num_samples=100, verbose=True):\n",
    "    \"\"\"\n",
    "    단일 텍스트에 대해 SHAP 분석 수행\n",
    "    \n",
    "    Args:\n",
    "        text: 분석할 원문 텍스트\n",
    "        reference_summary: 참조 요약 (없으면 모델로 생성)\n",
    "        num_samples: SHAP 샘플링 수\n",
    "        verbose: 자세한 출력 여부\n",
    "    \n",
    "    Returns:\n",
    "        shap_values: SHAP 값\n",
    "        summary: 생성된 요약\n",
    "        pipeline: 분석에 사용된 파이프라인 객체\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\n원문 분석 시작\\n{'='*80}\")\n",
    "        print(f\"원문 (일부): {text[:200]}...\")\n",
    "    \n",
    "    # 빈 텍스트 확인\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        raise ValueError(\"분석할 텍스트가 비어 있습니다.\")\n",
    "    \n",
    "    # 파이프라인 초기화 및 참조 설정\n",
    "    pipeline = SummarizationPipeline(model, tokenizer, embedder)\n",
    "    pipeline.set_reference(text, reference_summary)\n",
    "    \n",
    "    # SHAP Explainer 생성\n",
    "    try:\n",
    "        # 단어 단위로 분할\n",
    "        words = text.split()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP Explainer 초기화 중...\")\n",
    "            print(f\"텍스트 길이: {len(text)}, 단어 수: {len(words)}\")\n",
    "        \n",
    "        mask_token = tokenizer.pad_token or tokenizer.eos_token or \"…\"  # fallback\n",
    "        masker = shap.maskers.Text(tokenizer=tokenizer, mask_token=mask_token)          \n",
    "        \n",
    "        # Partition 마스커 사용 (더 안정적)\n",
    "        explainer = shap.Explainer(pipeline, masker)\n",
    "        \n",
    "        # 샘플 수 자동 조정 (너무 많으면 오래 걸림)\n",
    "        num_features = len(words)\n",
    "        adjusted_samples = min(max(2 * num_features + 1, 50), num_samples)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP 값 계산 중 (단어 수: {num_features}, 샘플 수: {adjusted_samples})...\")\n",
    "        \n",
    "        # SHAP 값 계산 - 단일 데이터로 명시\n",
    "        shap_values = explainer([text], max_evals=30)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"SHAP 분석 완료\")\n",
    "            print(f\"SHAP 값 형태: {shap_values.values.shape}\")\n",
    "        \n",
    "        return shap_values, pipeline.reference_summary, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"더 단순한 설정으로 SHAP 분석 재시도...\")\n",
    "        try:\n",
    "            # 더 단순하고 안정적인 접근방식 사용\n",
    "            # 텍스트 직접 처리\n",
    "            tokenized_text = text.split()\n",
    "            \n",
    "            # 수동으로 SHAP 값 생성\n",
    "            dummy_values = np.zeros((1, len(tokenized_text)))\n",
    "            \n",
    "            # 각 토큰의 중요도를 간단히 계산 (임베딩 유사도 기반)\n",
    "            original_summary = summarize_text(text)\n",
    "            \n",
    "            for i, token in enumerate(tokenized_text):\n",
    "                # 해당 토큰 제거\n",
    "                modified_text = ' '.join([t for j, t in enumerate(tokenized_text) if j != i])\n",
    "                \n",
    "                try:\n",
    "                    # 수정된 텍스트로 요약 생성\n",
    "                    modified_summary = summarize_text(modified_text)\n",
    "                    \n",
    "                    # 두 요약의 임베딩 비교\n",
    "                    orig_emb = pipeline.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "                    mod_emb = pipeline.embedder.encode(modified_summary, convert_to_tensor=True)\n",
    "                    \n",
    "                    # 유사도 계산 (1 - 유사도 = 중요도)\n",
    "                    similarity = util.cos_sim(orig_emb, mod_emb).item()\n",
    "                    importance = 1.0 - similarity\n",
    "                    \n",
    "                    # 중요도 저장\n",
    "                    dummy_values[0, i] = importance\n",
    "                except Exception as e2:\n",
    "                    print(f\"토큰 '{token}' 처리 중 오류: {e2}\")\n",
    "                    dummy_values[0, i] = 0.0\n",
    "            \n",
    "            # SHAP 결과와 유사한 객체 생성\n",
    "            class CustomShapValues:\n",
    "                def __init__(self, values, data, feature_names):\n",
    "                    self.values = values\n",
    "                    self.data = data\n",
    "                    self.feature_names = feature_names\n",
    "                    self.output_names = [\"importance\"]\n",
    "                    self.base_values = np.zeros(1)\n",
    "            \n",
    "            # 결과 반환\n",
    "            shap_obj = CustomShapValues(\n",
    "                values=dummy_values,\n",
    "                data=np.array([text]),\n",
    "                feature_names=tokenized_text\n",
    "            )\n",
    "            \n",
    "            print(\"대체 SHAP 분석 완료\")\n",
    "            return shap_obj, original_summary, pipeline\n",
    "            \n",
    "        except Exception as retry_error:\n",
    "            print(f\"대체 SHAP 분석도 실패: {retry_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise ValueError(\"모든 SHAP 분석 방법이 실패했습니다.\")\n",
    "\n",
    "def print_important_features(shap_values, summary, top_n=10):\n",
    "    \"\"\"\n",
    "    SHAP 값을 기반으로 중요 feature를 출력\n",
    "    \"\"\"\n",
    "    # SHAP 값의 절대값을 기준으로 정렬\n",
    "    token_importances = []\n",
    "    \n",
    "    try:\n",
    "        # 데이터 형식에 따라 다르게 처리\n",
    "        if hasattr(shap_values, 'data') and isinstance(shap_values.data, np.ndarray):\n",
    "            text_data = shap_values.data[0]\n",
    "            if isinstance(text_data, np.ndarray) and text_data.size == 1:\n",
    "                text_data = text_data.item()\n",
    "        else:\n",
    "            # 대체 방식\n",
    "            text_data = shap_values.data[0] if hasattr(shap_values, 'data') else \"텍스트 데이터 없음\"\n",
    "        \n",
    "        # 토큰 분할\n",
    "        tokens = text_data.split() if isinstance(text_data, str) else []\n",
    "        \n",
    "        # 토큰과 SHAP 값 매핑\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i < shap_values.values.shape[1]:  # 인덱스 범위 확인\n",
    "                importance = abs(shap_values.values[0][i])\n",
    "                raw_value = shap_values.values[0][i]\n",
    "                token_importances.append((token, importance, raw_value))\n",
    "        \n",
    "        # 중요도 기준 정렬\n",
    "        sorted_importances = sorted(token_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"생성된 요약: {summary}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"상위 {top_n}개 중요 단어 (SHAP 기준):\")\n",
    "        print(f\"{'단어':<15} | {'중요도':>10} | {'영향':>10} | {'해석'}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        for token, importance, raw_value in sorted_importances[:top_n]:\n",
    "            effect = \"긍정적 영향\" if raw_value > 0 else \"부정적 영향\"\n",
    "            print(f\"{token:<15} | {importance:>10.4f} | {raw_value:>10.4f} | {effect}\")\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"중요 특성 출력 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"간소화된 분석 결과 출력:\")\n",
    "        print(f\"요약: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로딩 중: ./data/train_original.json\n",
      "데이터셋 로딩 완료: 10 샘플\n",
      "\n",
      "총 10개 샘플 분석 시작\n",
      "\n",
      "샘플 1/10 분석:\n",
      "\n",
      "================================================================================\n",
      "원문 분석 시작\n",
      "================================================================================\n",
      "원문 (일부): 최명국 송하진 지사, 방중 주요 성과로 '군산~연운항' 항로 개설 협의 꼽아 장쑤성 당서기 \"바닷길 통한 협력, 적극 검토\" 두 지역 인적 교류 활발, 지난해 석도 항로 증편 송하진 전북도지사가 중국 장쑤성(강소성) 방문의 주요 성과로 중국 측과의 '군산~장쑤성 연운항' 항로 개설 협의를 꼽았다. 송하진 도지사는 1일 출입기자들과 만나 \"군산과 장쑤성 연운...\n",
      "제공된 참조 요약: 송하진 전북도지사가 중국 장쑤성을 방문해 러우 친지앤 당서기와 새만금 산단 5공구 공동투자 활용안에 대해 협의하고 군산과 장쑤성 연운항 간 신규 여객 항로 개설을 통한 협력방안을 논의했다.\n",
      "SHAP Explainer 초기화 중...\n",
      "텍스트 길이: 892, 단어 수: 219\n",
      "SHAP 값 계산 중 (단어 수: 219, 샘플 수: 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 SHAP perturbation 진행:   0%|          | 0/100 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 38])\n",
      "[DEBUG] input_ids (앞 30개): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 SHAP perturbation 진행:   1%|          | 1/100 [11:37<19:11:17, 697.75s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Warning: 입력 수(1)와 출력 수(2)가 다릅니다. 0.0으로 채움\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 SHAP perturbation 진행:   1%|          | 1/100 [00:00<00:00, 155344.59step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 595])\n",
      "[DEBUG] input_ids (앞 30개): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 595])\n"
     ]
    }
   ],
   "source": [
    "# 5. 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"./data/train_original.json\" # 실제 파일 경로로 변경\n",
    "    df = load_dataset(dataset_path, max_samples=10)  # 테스트용 10개만\n",
    "    \n",
    "    # 텍스트와 참조 요약 추출\n",
    "    texts = df[\"text\"].tolist()\n",
    "    references = df[\"summary\"].tolist() if \"summary\" in df.columns else [None] * len(texts)\n",
    "    \n",
    "    print(f\"\\n총 {len(texts)}개 샘플 분석 시작\")\n",
    "    \n",
    "    # 각 샘플에 대해 SHAP 분석 수행\n",
    "    for i, (text, reference) in enumerate(zip(texts, references)):\n",
    "        print(f\"\\n샘플 {i+1}/{len(texts)} 분석:\")\n",
    "        \n",
    "        try:\n",
    "            shap_values, summary, _ = analyze_with_shap(\n",
    "                text, \n",
    "                reference_summary=reference,\n",
    "                num_samples=5,  # 샘플링 수 조정 (높을수록 정확하지만 느림)\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # 중요 feature 출력\n",
    "            print_important_features(shap_values, summary, top_n=15)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"샘플 {i+1} 분석 중 오류 발생: {e}\")\n",
    "            print(\"이 샘플은 건너뜁니다.\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n모든 분석 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
