{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8028de4c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#debuggingìš© ì„¤ì •\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\n",
      "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ GPU: 1ê°œ\n",
      "í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.00 MB\n",
      "27B ëª¨ë¸ ë¡œë“œ ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15edad3f2624e8f8f8356fc3900828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: cuda:0)\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "model_name = \"google/gemma-3-27b-it\"  # Gemma 3 27B ëª¨ë¸ (instruction tuned)\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# CUDA ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ\")\n",
    "    print(f\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # íŒ¨ë”© í† í° ì„¤ì •\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ì‹œë„\n",
    "try:\n",
    "    print(f\"27B ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # ìë™ìœ¼ë¡œ ì ì ˆí•œ ì¥ì¹˜ì— í• ë‹¹\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"CPUë¡œ ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # CPUë¡œ ë‹¤ì‹œ ì‹œë„\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,\n",
    "        token=token\n",
    "    )\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})\")\n",
    "\n",
    "# sentence embedding ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸)\n",
    "print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ text generation í•¨ìˆ˜\n",
    "def summarize_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gemma ëª¨ë¸ìš© instruction í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
    "    prompt = f\"<start_of_turn>user\\nê²½ì œê¸ˆìœµ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ë‹¨ì–´ ë° ë‚´ìš©ì¸ ë¬´ì—‡ì¸ì§€ ê³ ë ¤í•´ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”.\\n\\nê¸°ì‚¬: {text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§• ë° ì…ë ¥ ì¤€ë¹„\n",
    "    max_input_length = 2048-128\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"[DEBUG] input_ids shape:\", inputs[\"input_ids\"].shape)\n",
    "    print(\"[DEBUG] input_ids (ì• 30ê°œ):\", inputs[\"input_ids\"][0][:30]) \n",
    "    \n",
    "    # ìƒì„± ì„¤ì •\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "    \n",
    "    # ìš”ì•½ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\">>> input shape:\", inputs[\"input_ids\"].shape)\n",
    "\n",
    "            output = model.generate(**inputs, **gen_config)  # GPUì—ì„œ ìƒì„±\n",
    "            if output.dim() > 1:\n",
    "                output = output[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš© (batch_size=1)\n",
    "\n",
    "            if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "                print(\"[!] generate() ê²°ê³¼ì— nan ë˜ëŠ” inf í¬í•¨ë¨ â€” ë””ì½”ë”© ì¤‘ë‹¨\")\n",
    "                return \"\"\n",
    "\n",
    "            output = output.to(\"cpu\")  # CPUë¡œ ì˜®ê²¨ì„œ ì•ˆì „í•˜ê²Œ ë””ì½”ë”©\n",
    "            full_response = tokenizer.decode(output, skip_special_tokens=False)  # âœ… ì—¬ê¸° ìˆ˜ì •!\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] generate() ì˜¤ë¥˜: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    # ì‘ë‹µ íŒŒì‹± (ëª¨ë¸ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ)\n",
    "    try:\n",
    "        # Gemma ì‘ë‹µ í˜•ì‹ì—ì„œ ëª¨ë¸ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "        if \"<start_of_turn>model\" in full_response:\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\\n\")[1]\n",
    "            if \"<end_of_turn>\" in response_parts:\n",
    "                summary = response_parts.split(\"<end_of_turn>\")[0].strip()\n",
    "            else:\n",
    "                summary = response_parts.strip()\n",
    "        else:\n",
    "            # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "            summary = full_response.replace(prompt, \"\").strip()\n",
    "            \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"ìš”ì•½ íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° ì‹œë„\n",
    "        return full_response.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°\n",
    "def load_dataset(file_path, max_samples=50):\n",
    "    \"\"\"JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë¬¸ì„œì™€ ìš”ì•½ ë°ì´í„°ì…‹ ë¡œë“œ\"\"\"\n",
    "    print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
    "        if \"documents\" in data:\n",
    "            # ë¬¸ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "            documents = data[\"documents\"]\n",
    "            processed_data = []\n",
    "            \n",
    "            for doc in documents:\n",
    "                # ì›ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (textëŠ” ë°°ì—´ êµ¬ì¡°)\n",
    "                original_text = \"\"\n",
    "                if \"text\" in doc:\n",
    "                    # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°\n",
    "                    for paragraph in doc[\"text\"]:\n",
    "                        for sent_obj in paragraph:\n",
    "                            if \"sentence\" in sent_obj:\n",
    "                                original_text += sent_obj[\"sentence\"] + \" \"\n",
    "                \n",
    "                # ìš”ì•½ë¬¸ ì¶”ì¶œ\n",
    "                summary = \"\"\n",
    "                if \"abstractive\" in doc and doc[\"abstractive\"]:\n",
    "                    summary = doc[\"abstractive\"][0] if isinstance(doc[\"abstractive\"], list) else doc[\"abstractive\"]\n",
    "                \n",
    "                # í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìš© ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "                processed_doc = {\n",
    "                    \"id\": doc.get(\"id\", \"\"),\n",
    "                    \"title\": doc.get(\"title\", \"\"),\n",
    "                    \"text\": original_text.strip(),\n",
    "                    \"summary\": summary,\n",
    "                    \"category\": doc.get(\"category\", \"\"),\n",
    "                    \"media_name\": doc.get(\"media_name\", \"\")\n",
    "                }\n",
    "                \n",
    "                processed_data.append(processed_doc)\n",
    "            \n",
    "            # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "            df = pd.DataFrame(processed_data)\n",
    "            \n",
    "        else:\n",
    "            # ë‹¤ë¥¸ êµ¬ì¡°ì˜ JSON ì²˜ë¦¬\n",
    "            print(\"í‘œì¤€ êµ¬ì¡°ê°€ ì•„ë‹Œ JSON íŒŒì¼ì…ë‹ˆë‹¤.\")\n",
    "            if isinstance(data, list):\n",
    "                df = pd.DataFrame(data)\n",
    "            else:\n",
    "                df = pd.DataFrame([data])\n",
    "            \n",
    "            # ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ ë° ë³€í™˜\n",
    "            if 'article' in df.columns and 'text' not in df.columns:\n",
    "                df['text'] = df['article']\n",
    "            if 'summary' not in df.columns and 'abstractive' in df.columns:\n",
    "                df['summary'] = df['abstractive']\n",
    "        \n",
    "        # ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ìš”ì•½ì´ ìˆëŠ” í–‰ ì œê±°\n",
    "        df = df.dropna(subset=['text'])\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        \n",
    "        # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ\n",
    "        if len(df) > max_samples:\n",
    "            df = df.sample(max_samples, random_state=42)\n",
    "        \n",
    "        print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(df)} ìƒ˜í”Œ\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±\n",
    "        print(\"ì˜ˆì‹œ ë°ì´í„° ì‚¬ìš©\")\n",
    "        example_data = {\n",
    "            'text': [\n",
    "                \"í•œêµ­ì€í–‰ì´ í†µí™”ì •ì±… ê²°ì • íšŒì˜ì—ì„œ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ ë™ê²°í–ˆë‹¤. í•œêµ­ì€í–‰ì€ ì§€ë‚œí•´ 4ë¶„ê¸° ì´í›„ ê³„ì†ëœ ê²½ê¸°ì¹¨ì²´ì˜ ì˜í–¥ìœ¼ë¡œ ê³ ìš© ì‹œì¥ì´ ìœ„ì¶•ë˜ê³  ìˆìœ¼ë©°, ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ëª©í‘œ ìˆ˜ì¤€ìœ¼ë¡œ ì•ˆì •ë˜ê³  ìˆë‹¤ê³  íŒë‹¨í–ˆë‹¤. ì‹œì¥ ì „ë¬¸ê°€ë“¤ì€ í•œêµ­ ê²½ì œì˜ íšŒë³µì„¸ê°€ ì˜ˆìƒë³´ë‹¤ ë”ë””ê²Œ ì§„í–‰ë˜ê³  ìˆì–´ ê¸ˆë¦¬ ì¸í•˜ ê°€ëŠ¥ì„±ë„ ìˆë‹¤ê³  ì „ë§í–ˆë‹¤.\",\n",
    "                \"ê¸ˆìœµìœ„ì›íšŒëŠ” ì˜¤ëŠ˜ ê°€ê³„ë¶€ì±„ ê´€ë¦¬ ë°©ì•ˆì„ ë°œí‘œí–ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ì´ë¶€ì±„ì›ë¦¬ê¸ˆìƒí™˜ë¹„ìœ¨(DSR) ê·œì œë¥¼ 40%ë¡œ ê°•í™”í•˜ê³ , ë‹¤ì£¼íƒìì— ëŒ€í•œ ì£¼íƒë‹´ë³´ëŒ€ì¶œ ì œí•œì„ í™•ëŒ€í•˜ëŠ” ê²ƒì´ë‹¤. ë˜í•œ ì‹¤ìˆ˜ìš”ìì— ëŒ€í•œ ëŒ€ì¶œ ì§€ì›ì€ í™•ëŒ€í•˜ë˜, íˆ¬ê¸° ëª©ì ì˜ ëŒ€ì¶œì—ëŠ” ì œì¬ë¥¼ ê°•í™”í•˜ëŠ” íˆ¬íŠ¸ë™ ì „ëµì„ í¼ì¹˜ê¸°ë¡œ í–ˆë‹¤.\"\n",
    "            ],\n",
    "            'summary': [\n",
    "                \"í•œêµ­ì€í–‰ì´ í†µí™”ì •ì±… ê²°ì • íšŒì˜ì—ì„œ ê¸°ì¤€ê¸ˆë¦¬ ë™ê²°, ê²½ê¸°ì¹¨ì²´ë¡œ ì¸í•œ ê³ ìš©ì‹œì¥ ìœ„ì¶•ê³¼ ë¬¼ê°€ ì•ˆì • íŒë‹¨\",\n",
    "                \"ê¸ˆìœµìœ„, ê°€ê³„ë¶€ì±„ ê´€ë¦¬ë°©ì•ˆ ë°œí‘œ - DSR 40% ê°•í™”, ë‹¤ì£¼íƒì ëŒ€ì¶œì œí•œ í™•ëŒ€, ì‹¤ìˆ˜ìš”ì ì§€ì› í™•ëŒ€\"\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAPìš© íŒŒì´í”„ë¼ì¸ ë° Explainer êµ¬ì„±\n",
    "class SummarizationPipeline:\n",
    "    def __init__(self, model, tokenizer, embedder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.original_text = None\n",
    "        self.reference_summary = None\n",
    "        self.perturbation_count = 0\n",
    "    \n",
    "    def set_reference(self, text, summary=None):\n",
    "        \"\"\"ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° ìš”ì•½ ì„¤ì •\"\"\"\n",
    "        self.original_text = text\n",
    "        self.perturbation_count = 0\n",
    "        \n",
    "        if summary is None:\n",
    "            # ì°¸ì¡° ìš”ì•½ì´ ì—†ìœ¼ë©´ ëª¨ë¸ë¡œ ìƒì„±\n",
    "            print(\"ì°¸ì¡° ìš”ì•½ ìƒì„± ì¤‘...\")\n",
    "            self.reference_summary = summarize_text(text)\n",
    "            print(f\"ìƒì„±ëœ ì°¸ì¡° ìš”ì•½: {self.reference_summary}\")\n",
    "        else:\n",
    "            self.reference_summary = summary\n",
    "            print(f\"ì œê³µëœ ì°¸ì¡° ìš”ì•½: {self.reference_summary}\")\n",
    "        \n",
    "        # ì°¸ì¡° ìš”ì•½ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°\n",
    "        self.reference_embedding = self.embedder.encode(\n",
    "            self.reference_summary, convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"\n",
    "        SHAPìš© í˜¸ì¶œ í•¨ìˆ˜. Perturbationëœ í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë°›ì•„ ê°ê°ì˜ ìš”ì•½ í’ˆì§ˆ ì ìˆ˜ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        batch_size = min(2, len(texts))  # ë°°ì¹˜ í¬ê¸° ì¤„ì„(ë©”ëª¨ë¦¬ ê³ ë ¤)\n",
    "        all_scores = []\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        # tqdm ì§„í–‰ ë°” ì„¤ì •\n",
    "        total = getattr(self, \"_expected_perturbations\", 100)\n",
    "        pbar = tqdm(total=total, desc=\"ğŸ” SHAP perturbation ì§„í–‰\", unit=\"step\", leave=True)\n",
    "        pbar.n = self.perturbation_count  # ì´ì „ì— ì§„í–‰ëœ ìˆ˜ ë°˜ì˜\n",
    "        pbar.refresh()\n",
    "         \n",
    "                \n",
    "        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_scores = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                # ë„˜íŒŒì´ ë°°ì—´ì´ë‚˜ í…ì„œ ì²˜ë¦¬\n",
    "                if isinstance(text, (np.ndarray, torch.Tensor)):\n",
    "                    text = text.tolist()\n",
    "\n",
    "                # ë¦¬ìŠ¤íŠ¸ â†’ ì²« ì›ì†Œ êº¼ëƒ„\n",
    "                if isinstance(text, list) and len(text) > 0:\n",
    "                    text = text[0]\n",
    "\n",
    "                # ë¬¸ìì—´ì¸ì§€ ìµœì¢… í™•ì¸\n",
    "                if not isinstance(text, str):\n",
    "                    print(f\"[!] í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŒ: {type(text)}\")\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                \n",
    "                # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬\n",
    "                if not text.strip():\n",
    "                    batch_scores.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # ìš”ì•½ ìƒì„±\n",
    "                    summary = summarize_text(text)\n",
    "                    \n",
    "                    if not summary.strip():\n",
    "                        print(\"[!] ìš”ì•½ ê²°ê³¼ê°€ ê³µë°±ì…ë‹ˆë‹¤. ì ìˆ˜ ê³„ì‚° ìƒëµ\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # ìƒì„±ëœ ìš”ì•½ì˜ ì„ë² ë”© ê³„ì‚°\n",
    "                    summary_embedding = self.embedder.encode(\n",
    "                        summary, convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "                    # NaN ì²´í¬ ë° ì²˜ë¦¬\n",
    "                    if torch.isnan(summary_embedding).any() or torch.isnan(self.reference_embedding).any():\n",
    "                        print(f\"[!] NaN detected in embeddings. ìš”ì•½ í…ìŠ¤íŠ¸: {summary}\")\n",
    "                        batch_scores.append(0.0)\n",
    "                        continue\n",
    "                    \n",
    "                    # ì°¸ì¡° ìš”ì•½ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
    "                    similarity = util.cos_sim(\n",
    "                        summary_embedding, self.reference_embedding\n",
    "                    ).item()\n",
    "                    \n",
    "                    # ì ìˆ˜ ë²”ìœ„ ë³´ì •\n",
    "                    normalized_score = max(0.0, min(1.0, float(similarity)))\n",
    "                    batch_scores.append(normalized_score)\n",
    "\n",
    "                    self.perturbation_count += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[!] ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}\")\n",
    "                    batch_scores.append(0.0)  # ì˜¤ë¥˜ ì‹œ 0ì  ì²˜ë¦¬\n",
    "            \n",
    "            all_scores.extend(batch_scores)\n",
    "                    \n",
    "            # ğŸ” ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€\n",
    "            print(f\"[DEBUG] texts ê¸¸ì´: {len(texts)}\")\n",
    "            print(f\"[DEBUG] í˜„ì¬ batch_texts ê¸¸ì´: {len(batch_texts)}\")\n",
    "            print(f\"[DEBUG] batch_scores ê¸¸ì´: {len(batch_scores)}\")\n",
    "            print(f\"[DEBUG] ëˆ„ì  all_scores ê¸¸ì´: {len(all_scores)}\")\n",
    "    \n",
    "        # ë¬´ì¡°ê±´ ì…ë ¥ ê°œìˆ˜ì™€ ì¶œë ¥ ê°œìˆ˜ ë§ì¶°ì£¼ê¸°\n",
    "        if len(all_scores) != len(texts):\n",
    "            print(f\"[!] Warning: ì…ë ¥ ìˆ˜({len(texts)})ì™€ ì¶œë ¥ ìˆ˜({len(all_scores)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤. 0.0ìœ¼ë¡œ ì±„ì›€\")\n",
    "            if len(all_scores) > len(texts):\n",
    "                # ë„ˆë¬´ ë§ì´ ë§Œë“  ê²½ìš° ìë¥´ê¸°\n",
    "                all_scores = all_scores[:len(texts)]\n",
    "            else:\n",
    "                # ë¶€ì¡±í•œ ê²½ìš° 0.0ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "                while len(all_scores) < len(texts):\n",
    "                    all_scores.append(0.0)\n",
    "\n",
    "        if len(texts) == 1:\n",
    "            return [float(all_scores[0])]  # SHAPì´ 1ê°œë§Œ ìš”ì²­í•œ ê²½ìš°\n",
    "        else:\n",
    "            return [float(s) for s in all_scores]\n",
    "    \n",
    "    def explain_specific_tokens(self, tokens, top_n=5):\n",
    "        \"\"\"\n",
    "        íŠ¹ì • í† í°ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        original_summary = summarize_text(self.original_text)\n",
    "        \n",
    "        for token in tokens:\n",
    "            # í•´ë‹¹ í† í°ì„ ì œê±°í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            removed_text = self.original_text.replace(token, \"\")\n",
    "            \n",
    "            # ì œê±° í›„ ìš”ì•½ ìƒì„±\n",
    "            removed_summary = summarize_text(removed_text)\n",
    "            \n",
    "            # ì›ë³¸ ìš”ì•½ê³¼ ì œê±° í›„ ìš”ì•½ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "            original_emb = self.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "            removed_emb = self.embedder.encode(removed_summary, convert_to_tensor=True)\n",
    "            \n",
    "            # ìœ ì‚¬ë„ ì°¨ì´ê°€ í´ìˆ˜ë¡ í•´ë‹¹ í† í°ì´ ì¤‘ìš”í•¨\n",
    "            similarity = util.cos_sim(original_emb, removed_emb).item()\n",
    "            importance = 1.0 - similarity\n",
    "            \n",
    "            results.append({\n",
    "                \"token\": token,\n",
    "                \"importance\": importance,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"removed_summary\": removed_summary\n",
    "            })\n",
    "            \n",
    "        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        results.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "\n",
    "def analyze_with_shap(text, reference_summary=None, num_samples=100, verbose=True):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ SHAP ë¶„ì„ ìˆ˜í–‰\n",
    "    \n",
    "    Args:\n",
    "        text: ë¶„ì„í•  ì›ë¬¸ í…ìŠ¤íŠ¸\n",
    "        reference_summary: ì°¸ì¡° ìš”ì•½ (ì—†ìœ¼ë©´ ëª¨ë¸ë¡œ ìƒì„±)\n",
    "        num_samples: SHAP ìƒ˜í”Œë§ ìˆ˜\n",
    "        verbose: ìì„¸í•œ ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        shap_values: SHAP ê°’\n",
    "        summary: ìƒì„±ëœ ìš”ì•½\n",
    "        pipeline: ë¶„ì„ì— ì‚¬ìš©ëœ íŒŒì´í”„ë¼ì¸ ê°ì²´\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\nì›ë¬¸ ë¶„ì„ ì‹œì‘\\n{'='*80}\")\n",
    "        print(f\"ì›ë¬¸ (ì¼ë¶€): {text[:200]}...\")\n",
    "    \n",
    "    # ë¹ˆ í…ìŠ¤íŠ¸ í™•ì¸\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        raise ValueError(\"ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì°¸ì¡° ì„¤ì •\n",
    "    pipeline = SummarizationPipeline(model, tokenizer, embedder)\n",
    "    pipeline.set_reference(text, reference_summary)\n",
    "    \n",
    "    # SHAP Explainer ìƒì„±\n",
    "    try:\n",
    "        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "        words = text.split()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP Explainer ì´ˆê¸°í™” ì¤‘...\")\n",
    "            print(f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}, ë‹¨ì–´ ìˆ˜: {len(words)}\")\n",
    "        \n",
    "        mask_token = tokenizer.pad_token or tokenizer.eos_token or \"â€¦\"  # fallback\n",
    "        masker = shap.maskers.Text(tokenizer=tokenizer, mask_token=mask_token)          \n",
    "        \n",
    "        # Partition ë§ˆìŠ¤ì»¤ ì‚¬ìš© (ë” ì•ˆì •ì )\n",
    "        explainer = shap.Explainer(pipeline, masker)\n",
    "        \n",
    "        # ìƒ˜í”Œ ìˆ˜ ìë™ ì¡°ì • (ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "        num_features = len(words)\n",
    "        adjusted_samples = min(max(2 * num_features + 1, 50), num_samples)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"SHAP ê°’ ê³„ì‚° ì¤‘ (ë‹¨ì–´ ìˆ˜: {num_features}, ìƒ˜í”Œ ìˆ˜: {adjusted_samples})...\")\n",
    "        \n",
    "        # SHAP ê°’ ê³„ì‚° - ë‹¨ì¼ ë°ì´í„°ë¡œ ëª…ì‹œ\n",
    "        shap_values = explainer([text], max_evals=30)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"SHAP ë¶„ì„ ì™„ë£Œ\")\n",
    "            print(f\"SHAP ê°’ í˜•íƒœ: {shap_values.values.shape}\")\n",
    "        \n",
    "        return shap_values, pipeline.reference_summary, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"ë” ë‹¨ìˆœí•œ ì„¤ì •ìœ¼ë¡œ SHAP ë¶„ì„ ì¬ì‹œë„...\")\n",
    "        try:\n",
    "            # ë” ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ì ‘ê·¼ë°©ì‹ ì‚¬ìš©\n",
    "            # í…ìŠ¤íŠ¸ ì§ì ‘ ì²˜ë¦¬\n",
    "            tokenized_text = text.split()\n",
    "            \n",
    "            # ìˆ˜ë™ìœ¼ë¡œ SHAP ê°’ ìƒì„±\n",
    "            dummy_values = np.zeros((1, len(tokenized_text)))\n",
    "            \n",
    "            # ê° í† í°ì˜ ì¤‘ìš”ë„ë¥¼ ê°„ë‹¨íˆ ê³„ì‚° (ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜)\n",
    "            original_summary = summarize_text(text)\n",
    "            \n",
    "            for i, token in enumerate(tokenized_text):\n",
    "                # í•´ë‹¹ í† í° ì œê±°\n",
    "                modified_text = ' '.join([t for j, t in enumerate(tokenized_text) if j != i])\n",
    "                \n",
    "                try:\n",
    "                    # ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "                    modified_summary = summarize_text(modified_text)\n",
    "                    \n",
    "                    # ë‘ ìš”ì•½ì˜ ì„ë² ë”© ë¹„êµ\n",
    "                    orig_emb = pipeline.embedder.encode(original_summary, convert_to_tensor=True)\n",
    "                    mod_emb = pipeline.embedder.encode(modified_summary, convert_to_tensor=True)\n",
    "                    \n",
    "                    # ìœ ì‚¬ë„ ê³„ì‚° (1 - ìœ ì‚¬ë„ = ì¤‘ìš”ë„)\n",
    "                    similarity = util.cos_sim(orig_emb, mod_emb).item()\n",
    "                    importance = 1.0 - similarity\n",
    "                    \n",
    "                    # ì¤‘ìš”ë„ ì €ì¥\n",
    "                    dummy_values[0, i] = importance\n",
    "                except Exception as e2:\n",
    "                    print(f\"í† í° '{token}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e2}\")\n",
    "                    dummy_values[0, i] = 0.0\n",
    "            \n",
    "            # SHAP ê²°ê³¼ì™€ ìœ ì‚¬í•œ ê°ì²´ ìƒì„±\n",
    "            class CustomShapValues:\n",
    "                def __init__(self, values, data, feature_names):\n",
    "                    self.values = values\n",
    "                    self.data = data\n",
    "                    self.feature_names = feature_names\n",
    "                    self.output_names = [\"importance\"]\n",
    "                    self.base_values = np.zeros(1)\n",
    "            \n",
    "            # ê²°ê³¼ ë°˜í™˜\n",
    "            shap_obj = CustomShapValues(\n",
    "                values=dummy_values,\n",
    "                data=np.array([text]),\n",
    "                feature_names=tokenized_text\n",
    "            )\n",
    "            \n",
    "            print(\"ëŒ€ì²´ SHAP ë¶„ì„ ì™„ë£Œ\")\n",
    "            return shap_obj, original_summary, pipeline\n",
    "            \n",
    "        except Exception as retry_error:\n",
    "            print(f\"ëŒ€ì²´ SHAP ë¶„ì„ë„ ì‹¤íŒ¨: {retry_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise ValueError(\"ëª¨ë“  SHAP ë¶„ì„ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def print_important_features(shap_values, summary, top_n=10):\n",
    "    \"\"\"\n",
    "    SHAP ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” featureë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    # SHAP ê°’ì˜ ì ˆëŒ€ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    token_importances = []\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬\n",
    "        if hasattr(shap_values, 'data') and isinstance(shap_values.data, np.ndarray):\n",
    "            text_data = shap_values.data[0]\n",
    "            if isinstance(text_data, np.ndarray) and text_data.size == 1:\n",
    "                text_data = text_data.item()\n",
    "        else:\n",
    "            # ëŒ€ì²´ ë°©ì‹\n",
    "            text_data = shap_values.data[0] if hasattr(shap_values, 'data') else \"í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ\"\n",
    "        \n",
    "        # í† í° ë¶„í• \n",
    "        tokens = text_data.split() if isinstance(text_data, str) else []\n",
    "        \n",
    "        # í† í°ê³¼ SHAP ê°’ ë§¤í•‘\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i < shap_values.values.shape[1]:  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸\n",
    "                importance = abs(shap_values.values[0][i])\n",
    "                raw_value = shap_values.values[0][i]\n",
    "                token_importances.append((token, importance, raw_value))\n",
    "        \n",
    "        # ì¤‘ìš”ë„ ê¸°ì¤€ ì •ë ¬\n",
    "        sorted_importances = sorted(token_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ìƒì„±ëœ ìš”ì•½: {summary}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ìƒìœ„ {top_n}ê°œ ì¤‘ìš” ë‹¨ì–´ (SHAP ê¸°ì¤€):\")\n",
    "        print(f\"{'ë‹¨ì–´':<15} | {'ì¤‘ìš”ë„':>10} | {'ì˜í–¥':>10} | {'í•´ì„'}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        for token, importance, raw_value in sorted_importances[:top_n]:\n",
    "            effect = \"ê¸ì •ì  ì˜í–¥\" if raw_value > 0 else \"ë¶€ì •ì  ì˜í–¥\"\n",
    "            print(f\"{token:<15} | {importance:>10.4f} | {raw_value:>10.4f} | {effect}\")\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ì¤‘ìš” íŠ¹ì„± ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"ê°„ì†Œí™”ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥:\")\n",
    "        print(f\"ìš”ì•½: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: ./data/train_original.json\n",
      "ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: 10 ìƒ˜í”Œ\n",
      "\n",
      "ì´ 10ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘\n",
      "\n",
      "ìƒ˜í”Œ 1/10 ë¶„ì„:\n",
      "\n",
      "================================================================================\n",
      "ì›ë¬¸ ë¶„ì„ ì‹œì‘\n",
      "================================================================================\n",
      "ì›ë¬¸ (ì¼ë¶€): ìµœëª…êµ­ ì†¡í•˜ì§„ ì§€ì‚¬, ë°©ì¤‘ ì£¼ìš” ì„±ê³¼ë¡œ 'êµ°ì‚°~ì—°ìš´í•­' í•­ë¡œ ê°œì„¤ í˜‘ì˜ ê¼½ì•„ ì¥ì‘¤ì„± ë‹¹ì„œê¸° \"ë°”ë‹·ê¸¸ í†µí•œ í˜‘ë ¥, ì ê·¹ ê²€í† \" ë‘ ì§€ì—­ ì¸ì  êµë¥˜ í™œë°œ, ì§€ë‚œí•´ ì„ë„ í•­ë¡œ ì¦í¸ ì†¡í•˜ì§„ ì „ë¶ë„ì§€ì‚¬ê°€ ì¤‘êµ­ ì¥ì‘¤ì„±(ê°•ì†Œì„±) ë°©ë¬¸ì˜ ì£¼ìš” ì„±ê³¼ë¡œ ì¤‘êµ­ ì¸¡ê³¼ì˜ 'êµ°ì‚°~ì¥ì‘¤ì„± ì—°ìš´í•­' í•­ë¡œ ê°œì„¤ í˜‘ì˜ë¥¼ ê¼½ì•˜ë‹¤. ì†¡í•˜ì§„ ë„ì§€ì‚¬ëŠ” 1ì¼ ì¶œì…ê¸°ìë“¤ê³¼ ë§Œë‚˜ \"êµ°ì‚°ê³¼ ì¥ì‘¤ì„± ì—°ìš´...\n",
      "ì œê³µëœ ì°¸ì¡° ìš”ì•½: ì†¡í•˜ì§„ ì „ë¶ë„ì§€ì‚¬ê°€ ì¤‘êµ­ ì¥ì‘¤ì„±ì„ ë°©ë¬¸í•´ ëŸ¬ìš° ì¹œì§€ì•¤ ë‹¹ì„œê¸°ì™€ ìƒˆë§Œê¸ˆ ì‚°ë‹¨ 5ê³µêµ¬ ê³µë™íˆ¬ì í™œìš©ì•ˆì— ëŒ€í•´ í˜‘ì˜í•˜ê³  êµ°ì‚°ê³¼ ì¥ì‘¤ì„± ì—°ìš´í•­ ê°„ ì‹ ê·œ ì—¬ê° í•­ë¡œ ê°œì„¤ì„ í†µí•œ í˜‘ë ¥ë°©ì•ˆì„ ë…¼ì˜í–ˆë‹¤.\n",
      "SHAP Explainer ì´ˆê¸°í™” ì¤‘...\n",
      "í…ìŠ¤íŠ¸ ê¸¸ì´: 892, ë‹¨ì–´ ìˆ˜: 219\n",
      "SHAP ê°’ ê³„ì‚° ì¤‘ (ë‹¨ì–´ ìˆ˜: 219, ìƒ˜í”Œ ìˆ˜: 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 38])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   1%|          | 1/100 [11:13<18:31:52, 673.86s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] texts ê¸¸ì´: 1\n",
      "[DEBUG] í˜„ì¬ batch_texts ê¸¸ì´: 1\n",
      "[DEBUG] batch_scores ê¸¸ì´: 1\n",
      "[DEBUG] ëˆ„ì  all_scores ê¸¸ì´: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 595])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 595])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   2%|â–         | 2/100 [11:14<9:10:34, 337.09s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] texts ê¸¸ì´: 1\n",
      "[DEBUG] í˜„ì¬ batch_texts ê¸¸ì´: 1\n",
      "[DEBUG] batch_scores ê¸¸ì´: 1\n",
      "[DEBUG] ëˆ„ì  all_scores ê¸¸ì´: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input_ids shape: torch.Size([1, 306])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 306])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” SHAP perturbation ì§„í–‰:   3%|â–         | 3/100 [11:11<6:01:44, 223.76s/step]\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6468/2163360236.py\", line 218, in analyze_with_shap\n",
      "    shap_values = explainer([text], max_evals=30)\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 173, in __call__\n",
      "    return super().__call__(\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/explainers/_explainer.py\", line 366, in __call__\n",
      "    row_result = self.explain_row(\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 229, in explain_row\n",
      "    self.owen(fm, self._curr_base_value, f11, max_evals - 2, outputs, fixed_context, batch_size, silent)\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 339, in owen\n",
      "    fout = fm(batch_masks)\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py\", line 67, in __call__\n",
      "    return self._full_masking_call(masks, batch_size=batch_size)\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py\", line 143, in _full_masking_call\n",
      "    _assert_output_input_match(joined_masked_inputs, outputs)\n",
      "  File \"/home/elicer/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py\", line 282, in _assert_output_input_match\n",
      "    assert len(outputs) == len(inputs[0]), (\n",
      "AssertionError: The model produced 1 output rows when given 2 input rows! Check the implementation of the model you provided for errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] texts ê¸¸ì´: 1\n",
      "[DEBUG] í˜„ì¬ batch_texts ê¸¸ì´: 1\n",
      "[DEBUG] batch_scores ê¸¸ì´: 1\n",
      "[DEBUG] ëˆ„ì  all_scores ê¸¸ì´: 1\n",
      "SHAP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: The model produced 1 output rows when given 2 input rows! Check the implementation of the model you provided for errors.\n",
      "ë” ë‹¨ìˆœí•œ ì„¤ì •ìœ¼ë¡œ SHAP ë¶„ì„ ì¬ì‹œë„...\n",
      "[DEBUG] input_ids shape: torch.Size([1, 595])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 595])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 587])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 587])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 587])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 587])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 594])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 588])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 588])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 591])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 593])\n",
      "[DEBUG] input_ids shape: torch.Size([1, 592])\n",
      "[DEBUG] input_ids (ì• 30ê°œ): tensor([     2,    105,   2364,    107, 179886, 200086, 234416,   9554,  68564,\n",
      "         23591, 239114, 219770, 236761,  86394,  29950, 237430,  24566,  54422,\n",
      "        237558,  94485,  93860, 143495,  22063,  23591, 239114, 219770, 236761,\n",
      "           108, 237351, 237470], device='cuda:0')\n",
      ">>> input shape: torch.Size([1, 592])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 218\u001b[0m, in \u001b[0;36manalyze_with_shap\u001b[0;34m(text, reference_summary, num_samples, verbose)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# SHAP ê°’ ê³„ì‚° - ë‹¨ì¼ ë°ì´í„°ë¡œ ëª…ì‹œ\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/explainers/_partition.py:173\u001b[0m, in \u001b[0;36mPartitionExplainer.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/explainers/_explainer.py:366\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 366\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/explainers/_partition.py:229\u001b[0m, in \u001b[0;36mPartitionExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(out_shape)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mowen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_curr_base_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# if False:\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m#     if self.multi_output:\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#         return [self.dvalues[:,i] for i in range(self.dvalues.shape[1])], oinds\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# drop the interaction terms down onto self.values\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/explainers/_partition.py:339\u001b[0m, in \u001b[0;36mPartitionExplainer.owen\u001b[0;34m(self, fm, f00, f11, max_evals, output_indexes, fixed_context, batch_size, silent)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 339\u001b[0m     fout \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_indexes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:67\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:143\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    142\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mjoined_masked_inputs)\n\u001b[0;32m--> 143\u001b[0m \u001b[43m_assert_output_input_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:282\u001b[0m, in \u001b[0;36m_assert_output_input_match\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_assert_output_input_match\u001b[39m(inputs, outputs):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m]), (\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model produced \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output rows when given \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input rows! Check the implementation of the model you provided for errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model produced 1 output rows when given 2 input rows! Check the implementation of the model you provided for errors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mìƒ˜í”Œ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ë¶„ì„:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     shap_values, summary, _ \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_with_shap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ìƒ˜í”Œë§ ìˆ˜ ì¡°ì • (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# ì¤‘ìš” feature ì¶œë ¥\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     print_important_features(shap_values, summary, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 249\u001b[0m, in \u001b[0;36manalyze_with_shap\u001b[0;34m(text, reference_summary, num_samples, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m modified_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t \u001b[38;5;28;01mfor\u001b[39;00m j, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokenized_text) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m i])\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ìƒì„±\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     modified_summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# ë‘ ìš”ì•½ì˜ ì„ë² ë”© ë¹„êµ\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     orig_emb \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39mencode(original_summary, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> input shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 35\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GPUì—ì„œ ìƒì„±\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     37\u001b[0m         output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš© (batch_size=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/generation/utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3293\u001b[0m     outputs,\n\u001b[1;32m   3294\u001b[0m     model_kwargs,\n\u001b[1;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3296\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:1351\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mignore_index, labels)\n\u001b[1;32m   1348\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_causal_mask(\n\u001b[1;32m   1349\u001b[0m     attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n\u001b[1;32m   1350\u001b[0m )\n\u001b[0;32m-> 1351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:978\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:756\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    743\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    744\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m         last_cache_position,\n\u001b[1;32m    754\u001b[0m     )\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 756\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:461\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    460\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    463\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:119\u001b[0m, in \u001b[0;36mGemma3MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 119\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"./data/train_original.json\" # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½\n",
    "    df = load_dataset(dataset_path, max_samples=10)  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° ìš”ì•½ ì¶”ì¶œ\n",
    "    texts = df[\"text\"].tolist()\n",
    "    references = df[\"summary\"].tolist() if \"summary\" in df.columns else [None] * len(texts)\n",
    "    \n",
    "    print(f\"\\nì´ {len(texts)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘\")\n",
    "    \n",
    "    # ê° ìƒ˜í”Œì— ëŒ€í•´ SHAP ë¶„ì„ ìˆ˜í–‰\n",
    "    for i, (text, reference) in enumerate(zip(texts, references)):\n",
    "        print(f\"\\nìƒ˜í”Œ {i+1}/{len(texts)} ë¶„ì„:\")\n",
    "        \n",
    "        try:\n",
    "            shap_values, summary, _ = analyze_with_shap(\n",
    "                text, \n",
    "                reference_summary=reference,\n",
    "                num_samples=5,  # ìƒ˜í”Œë§ ìˆ˜ ì¡°ì • (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # ì¤‘ìš” feature ì¶œë ¥\n",
    "            print_important_features(shap_values, summary, top_n=15)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ìƒ˜í”Œ {i+1} ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"ì´ ìƒ˜í”Œì€ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nëª¨ë“  ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
